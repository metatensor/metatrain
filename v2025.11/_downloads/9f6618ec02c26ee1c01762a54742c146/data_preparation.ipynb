{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to prepare data for training\n\n.. attention::\n\n    This tutorial is only relevant for users who need to prepare their data from scratch\n    from several files or for big datasets. If you already have your data in a common\n    file format (like XYZ or `ASE database`_), you can skip this tutorial and directly\n    start training.\n\n\nXYZ, ASE databases, and also from metrain's\n:class:`metatrain.utils.data.dataset.DiskDataset <DiskDataset>` file.\n\nFor the small datasets (<10k structures), you can simply provide an XYZ file or an ASE\ndatabase to ``metatrain``, and it will handle the data loading for you. Large datasets\n(>10k structures) may not fit into the GPU memory. In such cases, it is useful to\npre-process the dataset, save it to disk and load it on the fly during training.\n\nIn this tutorial, we will show how to prepare data for training using three different\nformats. You can choose the one that best fits your needs.\n\nWe start by importing the necessary packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import subprocess\nfrom pathlib import Path\n\nimport ase.io\nimport numpy as np\nimport torch\nfrom metatensor.torch import Labels, TensorBlock, TensorMap\nfrom metatomic.torch import NeighborListOptions, systems_to_torch\n\nfrom metatrain.utils.data.writers import DiskDatasetWriter\nfrom metatrain.utils.neighbor_lists import get_system_with_neighbor_lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a XYZ training file (small datasets)\n\nFirst, we will show how to create a XYZ file with fields corresponding to the target\nproperties. On modern HPC systems, this format is suitable for datasets up to around\n1M structures. As an example, we will use 100 structures from a file read by ASE_.\nSince files from reference calculations may be located in different directories, we\nfirst create a list of all path that we want to read from. Here, for simplicity, we\nassume that all files are located in the same directory.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filelist = 100 * [\"qm9_reduced_100.xyz\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now read the structures using the ASE package. Check the ase documentation for\nmore details on how to read different file formats. Instead of creating the ``atoms``\nobject by reading from disk, you can also create an\n:class:`ase.Atoms` object containing the chemical ``symbols``, ``positions``, the\n``cell`` and the periodic boundary conditions (``pbc``) by hand using its constructor.\n\n.. hint::\n\n  If a property is not read by the :func:`ase.io.read` function, you can add custom\n  scalar properties to the ``info`` dictionary. Vector properties (e.g. forces) can be\n  added to the ``arrays`` dictionary. Tensor properties (e.g. stress) must\n  be flattened before adding them to the ``arrays`` dictionary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "frames = []\nfor i, fname in enumerate(filelist):\n    atoms = ase.io.read(fname, index=i)\n\n    n_atoms = len(atoms)\n    # scalar\n    atoms.info[\"U0\"] = -100.0\n    # vector\n    atoms.arrays[\"forces\"] = np.zeros((n_atoms, 3))\n    # tensor\n    atoms.arrays[\"my_tensor\"] = np.zeros((n_atoms, 3, 3)).reshape(n_atoms, 9)\n\n    frames.append(atoms)\n\nase.io.write(\"data.xyz\", frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The names of the added properties (like, ``U0``, etc.) must be referenced correctly\n  in the ``options.yaml`` file.</p></div>\n\n## Create a ``DiskDataset`` (large datasets)\n\nIn addition to the systems and targets (as above), we also save the neighbor\nlists that the model will use during training. We first create the writer object that\nwill write the data to a zip file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "disk_dataset_writer = DiskDatasetWriter(\"qm9_reduced_100.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we loop over all structures, convert them to the internal torch format using\n:func:`metatomic.torch.systems_to_torch`, compute the neighbor lists using\n:func:`metatrain.utils.neighbor_lists.get_system_with_neighbor_lists` and write\neverything to disk using the writer's ``write()`` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, fname in enumerate(filelist):\n    atoms = ase.io.read(fname, index=i)\n\n    system = systems_to_torch(atoms, dtype=torch.float64)\n    system = get_system_with_neighbor_lists(\n        system,\n        [NeighborListOptions(cutoff=5.0, full_list=True, strict=True)],\n    )\n    energy = TensorMap(\n        keys=Labels.single(),\n        blocks=[\n            TensorBlock(\n                values=torch.tensor([[atoms.info[\"U0\"]]], dtype=torch.float64),\n                samples=Labels(\n                    names=[\"system\"],\n                    values=torch.tensor([[i]]),\n                ),\n                components=[],\n                properties=Labels(\"energy\", torch.tensor([[0]])),\n            )\n        ],\n    )\n    disk_dataset_writer.write([system], {\"energy\": energy})\n\ndisk_dataset_writer.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, you can also write the whole dataset at once, which might be more\nefficient (but also potentially run into memory issues). We use the same ``frames``\nthat we created above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "disk_dataset_writer = DiskDatasetWriter(\"qm9_reduced_100_all_at_once.zip\")\n\nsystems = systems_to_torch(frames, dtype=torch.float64)\nsystems = [\n    get_system_with_neighbor_lists(\n        system,\n        [NeighborListOptions(cutoff=5.0, full_list=True, strict=True)],\n    )\n    for system in systems\n]\nenergy = TensorMap(\n    keys=Labels.single(),\n    blocks=[\n        TensorBlock(\n            values=torch.tensor(\n                [frame.info[\"U0\"] for frame in frames], dtype=torch.float64\n            ).reshape(-1, 1),\n            samples=Labels.range(\"system\", len(frames)),\n            components=[],\n            properties=Labels(\"energy\", torch.tensor([[0]])),\n        )\n    ],\n)\n\ndisk_dataset_writer.write(systems, {\"energy\": energy})\ndisk_dataset_writer.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is saved to disk. You can now provide it to ``metatrain`` as a\ndataset to train from, simply by replacing your ``.xyz`` file with the newly created\nzip file (e.g. ``read_from: qm9_reduced_100.zip``).\n\n## Create a ``MemmapDataset`` (large datasets, parallel filesystems)\n\nIf your dataset is large and you are using a parallel filesystem (e.g. on an HPC\ncluster), it is recommended to use a ``MemmapDataset`` instead of a ``DiskDataset``.\nThe ``MemmapDataset`` stores the data inside memory-mapped numpy arrays instead of a\nzip file. Reading from this format avoids I/O bottlenecks, but it does not support\nspherical targets or storing neighbor lists.\n\nAs an example, we will use 100 structures from a dataset of carbon structures. The\nnumpy arrays must be saved inside a directory, using the following format.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "structures = ase.io.read(\"carbon_reduced_100.xyz\", index=\":\")\n\nroot = Path(\"carbon_reduced_100_memmap/\")\nroot.mkdir()\n\nns_path = root / \"ns.npy\"\nna_path = root / \"na.npy\"\na_path = root / \"a.bin\"\nx_path = root / \"x.bin\"\nc_path = root / \"c.bin\"\ne_path = root / \"e.bin\"\nf_path = root / \"f.bin\"\ns_path = root / \"s.bin\"\n\nns = len(structures)\nna = np.cumsum(np.array([0] + [len(s) for s in structures], dtype=np.int64))\nnp.save(ns_path, ns)\nnp.save(na_path, na)\n\na_mm = np.memmap(a_path, dtype=\"int32\", mode=\"w+\", shape=(na[-1],))\nx_mm = np.memmap(x_path, dtype=\"float32\", mode=\"w+\", shape=(na[-1], 3))\nc_mm = np.memmap(c_path, dtype=\"float32\", mode=\"w+\", shape=(ns, 3, 3))\ne_mm = np.memmap(e_path, dtype=\"float32\", mode=\"w+\", shape=(ns, 1))\nf_mm = np.memmap(f_path, dtype=\"float32\", mode=\"w+\", shape=(na[-1], 3))\ns_mm = np.memmap(s_path, dtype=\"float32\", mode=\"w+\", shape=(ns, 3, 3))\n\nfor i, s in enumerate(structures):\n    a_mm[na[i] : na[i + 1]] = s.numbers\n    x_mm[na[i] : na[i + 1]] = s.get_positions()\n    c_mm[i] = s.get_cell()[:]\n    e_mm[i] = s.get_potential_energy()\n    f_mm[na[i] : na[i + 1]] = s.arrays[\"force\"]\n    s_mm[i] = -s.info[\"virial\"] / s.get_volume()\n\na_mm.flush()\nx_mm.flush()\nc_mm.flush()\ne_mm.flush()\nf_mm.flush()\ns_mm.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is saved to disk. You can now provide it to ``metatrain`` as a\ndataset to train from, simply by specifying the newly created\ndirectory as the path from which to read the systems\n(e.g. ``read_from: carbon_reduced_100_memmap/``).\n\nFor example, you can use the following options file:\n\n.. literalinclude:: options.yaml\n   :language: yaml\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run([\"mtt\", \"train\", \"options.yaml\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
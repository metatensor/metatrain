{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Transfer Learning (experimental)\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This section of the documentation is only relevant for PET model so far.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Features described in this section are experimental and not yet\n  extensively tested. Please use them at your own risk and report any\n  issues you encounter to the developers. The transfer learned models\n  cannot be used in MD engines such as ASE or LAMMPS yet.</p></div>\n\n\nThis section describes the process of transfer learning, which is a\ncommon technique used in machine learning, where a model is pre-trained on\nthe dataset with one level of theory and/or one set of properties and then\nfine-tuned on a different dataset with a different level of theory and/or\ndifferent set of properties. This approach to use the learned representations\nfrom the pre-trained model and adapt them to the targets, which can be\nexpensive to compute and/or not available in the pre-trained dataset.\n\nIn the following sections we assume that the pre-trained model is trained on the\nconventional DFT dataset with energies, forces and stresses, which are provided\nas ``energy`` targets (and its derivatives) in the ``options.yaml`` file.\n\n\n## Fitting to a new level of theory\n\nTraining on a new level of theory is a common use case for transfer learning. It\nrequires using a pre-trained model checkpoint with the ``mtt train`` command and setting\nthe new targets corresponding to the new level of theory in the ``options.yaml`` file.\nLet's assume that the training is done on the dataset computed with the hybrid DFT\nfunctional (e.g. PBE0) stored in the ``new_train_dataset.xyz`` file, where the\ncorresponsing energies and forces are written in the ``energy`` and ``forces`` key of\nthe ``info`` dictionary of the ``ase.Atoms`` object. Then, the ``options.yaml`` file\nshould look like this:\n\n```yaml\narchitecture:\n  name: pet\n  training:\n    finetune:\n      method: full\n      read_from: path/to/checkpoint.ckpt\n\ntraining_set:\n  systems:\n    read_from: dataset.xyz\n    reader: ase\n    length_unit: angstrom\n  targets:\n    mtt::energy_pbe0: # name of the new target\n      key: energy # key of the target in the atoms.info dictionary\n      unit: eV # unit of the target value\n      forces:\n        key: forces\n\ntest_set: 0.1\nvalidation_set: 0.1\n```\nThe validation and test sets can be set in the same way. The training\nprocess will then create a new composition model and new heads for the\ntarget ``mtt::energy_pbe0``. The rest of the model weights will be\ninitialized from the pre-trained model checkpoint.\n\n## Fitting to a new set of properties\n\nTraining on a new set of properties is another common use case for transfer learning. It\ncan be done in a similar way as training on a new level of theory. The only difference\nis that the new targets need to be properly set in the ``options.yaml`` file. More\ninformation about fitting the generic targets can be found in the `Fitting generic\ntargets <fitting-generic-targets>` section of the documentation.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
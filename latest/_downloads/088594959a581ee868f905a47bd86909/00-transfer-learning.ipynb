{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Transfer Learning (experimental)\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This section of the documentation is only relevant for PET model so far.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Features described in this section are experimental and not yet\n  extensively tested. Please use them at your own risk and report any\n  issues you encounter to the developers. The transfer learned models\n  cannot be directly used in MD engines such as ASE or LAMMPS yet.\n  If you still want to use them, please follow the instructions below.</p></div>\n\n\nThis section describes the process of transfer learning, which is a\ncommon technique used in machine learning, where a model is pre-trained on\nthe dataset with one level of theory and/or one set of properties and then\nfine-tuned on a different dataset with a different level of theory and/or\ndifferent set of properties. This approach to use the learned representations\nfrom the pre-trained model and adapt them to the targets, which can be\nexpensive to compute and/or not available in the pre-trained dataset.\n\nIn the following sections we assume that the pre-trained model is trained on the\nconventional DFT dataset with energies, forces and stresses, which are provided\nas ``energy`` targets (and its derivatives) in the ``options.yaml`` file.\n\n\n## Fitting to a new level of theory\n\nTraining on a new level of theory is a common use case for transfer learning. It\nrequires using a pre-trained model checkpoint with the ``mtt train`` command and setting\nthe new targets corresponding to the new level of theory in the ``options.yaml`` file.\nLet's assume that the training is done on the dataset computed with the hybrid DFT\nfunctional (e.g. PBE0) stored in the ``new_train_dataset.xyz`` file, where the\ncorresponsing energies and forces are written in the ``energy`` and ``forces`` key of\nthe ``info`` dictionary of the ``ase.Atoms`` object. Then, the ``options.yaml`` file\nshould look like this:\n\n```yaml\narchitecture:\n  name: pet\n  training:\n    finetune:\n      method: full\n      read_from: path/to/checkpoint.ckpt\n\ntraining_set:\n  systems:\n    read_from: dataset.xyz\n    reader: ase\n    length_unit: angstrom\n  targets:\n    mtt::energy_pbe0: # name of the new target\n      key: energy # key of the target in the atoms.info dictionary\n      unit: eV # unit of the target value\n      forces:\n        key: forces\n\ntest_set: 0.1\nvalidation_set: 0.1\n```\nThe validation and test sets can be set in the same way. The training\nprocess will then create a new composition model and new heads for the\ntarget ``mtt::energy_pbe0``. The rest of the model weights will be\ninitialized from the pre-trained model checkpoint.\n\n## Inheriting weights from existing heads\n\nIn some cases, the new targets might be similar to the existing targets\nin the pre-trained model. For example, if the pre-trained model is trained\non energies and forces computed with the PBE functional, and the new targets\nare energies and forces coming from the PBE0 calculations, it might be beneficial\nto initialize the new PBE0 heads and last layers with the weights of the PBE\nheads and last layers. This can be done by specifying the ``inherit_heads``\nparameter in the ``options.yaml`` file:\n\n```yaml\narchitecture:\n  training:\n    finetune:\n      method: full\n      read_from: path/to/checkpoint.ckpt\n      inherit_heads:\n        mtt::energy_pbe0: energy # inherit weights from the \"energy\" head\n```\nThe ``inherit_heads`` parameter is a dictionary mapping the new trainable\ntargets specified in the ``training_set/targets`` section to the existing\ntargets in the pre-trained model. The weights of the corresponding heads and\nlast layers will be copied from the source heads to the destination heads\ninstead of random initialization. These weights are still trainable and\nwill be adapted to the new dataset during the training process.\n\n\n## Using the transfer-learned model in simulation engines\n\nThe default target name expected by the ``metatomic`` package in order\nto use the model in ASE and LAMMPS calculations is ``energy``. If the new\ntransfer-learned target has a different name, e.g. ``mtt::energy_pbe0``,\nthe ``metatomic`` model interface will still try to access the ``energy``\ntarget name while evaluating energies and forces. Currently, there is no\nautomatic way to set the target name in the ``metatomic`` model interface\n(this feature is under development). Therefore, in order to use the\ntransfer-learned model in simulation engines, the new target needs to be renamed\nto ``energy`` in the trained model checkpoint ``.ckpt`` file. This can be done\nusing a relatively simple python script:\n\n```python\nimport metatomic.torch\nimport torch\n\n\ndef set_output_head(checkpoint, head_name):\n    for state_dict_name in [\"model_state_dict\", \"best_model_state_dict\"]:\n        state_dict = checkpoint.get(state_dict_name)\n        if state_dict is not None:\n            new_state_dict = {}\n            for key, value in state_dict.items():\n                if \".energy.\" in key:\n                    continue\n                if \"scaler.scales\" in key:\n                    value = value[:1]\n                if head_name in key:\n                    new_key = key.replace(head_name, \"energy\")\n                else:\n                    new_key = key\n                new_state_dict[new_key] = value\n            checkpoint[state_dict_name] = new_state_dict\n    dataset_info = checkpoint[\"model_data\"][\"dataset_info\"]\n    if dataset_info is not None:\n        new_target = dataset_info.targets.pop(head_name)\n        if new_target is not None:\n            dataset_info.targets[\"energy\"] = new_target\n            checkpoint[\"model_data\"][\"dataset_info\"] = dataset_info\n    return checkpoint\n\n\ncheckpoint = torch.load(\n    \"your_path_to_checkpoint/model.ckpt\", map_location=\"cpu\", weights_only=False\n)\nnew_target_name = \"mtt::energy_pbe0\"  # specify the name of the new target here\ncheckpoint = set_output_head(checkpoint, new_target_name)\ntorch.save(checkpoint, \"new_checkpoint.ckpt\")\n```\nYou need to specify the path to the trained model checkpoint and the name of the new\ntarget in the script. This name should match the new target name in the\n``options.yaml`` file. The modified checkpoint will be saved as ``new_checkpoint.ckpt``.\nFinally, you can run the ``mtt export new_checkpoint.ckpt`` command to convert the\nmodel to the ``metatomic`` format and use it in ASE and LAMMPS calculations, as\ndescribed in the `metatomic documentation`_.\n\n\n\n## Fitting to a new set of properties\n\nTraining on a new set of properties is another common use case for transfer learning. It\ncan be done in a similar way as training on a new level of theory. The only difference\nis that the new targets need to be properly set in the ``options.yaml`` file. More\ninformation about fitting the generic targets can be found in the `Fitting generic\ntargets <fitting-generic-targets>` section of the documentation.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
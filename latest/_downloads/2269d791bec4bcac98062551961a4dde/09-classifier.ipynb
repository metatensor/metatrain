{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a Classifier Model\n\nThis tutorial demonstrates how to train a classifier model using metatrain.\nThe classifier model is a transfer learning architecture that takes a pre-trained\nmodel, freezes its backbone, and trains a small multi-layer perceptron (MLP) on\ntop of the extracted features for classification tasks.\n\nIn this example, we will classify carbon allotropes (diamond, graphite, and graphene).\n\n## Creating the Dataset\n\nFirst, we need to create a dataset with different carbon structures. We'll generate\nsimple structures for diamond, graphite, and graphene, and label them with one-hot\nencoded class labels. The classifier also supports soft/fractional targets for cases\nwhere the class membership is uncertain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import subprocess\n\nimport ase.io\nimport chemiscope\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ase.build import bulk, graphene\nfrom metatomic.torch import ModelOutput\nfrom metatomic.torch.ase_calculator import MetatomicCalculator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper function to convert class labels to one-hot encodings\n\nThis function converts string class labels (e.g., \"diamond\", \"graphite\", \"graphene\")\ninto one-hot encoded probability vectors that the classifier can use for training.\nYou can use it yourself to prepare datasets if you're starting from string labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def class_to_onehot(class_label: str, class_names: list[str]) -> list[float]:\n    \"\"\"Convert a class label string to a one-hot encoded probability vector.\n\n    :param class_label: The class label as a string (e.g., \"diamond\")\n    :param class_names: List of all possible class names in order\n    :return: One-hot encoded probability vector (e.g., [1.0, 0.0, 0.0])\n    \"\"\"\n    if class_label not in class_names:\n        raise ValueError(f\"Unknown class label: {class_label}\")\n\n    onehot = [0.0] * len(class_names)\n    onehot[class_names.index(class_label)] = 1.0\n    return onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We generate structures for three carbon allotropes:\n\n- Diamond (class 0)\n- Graphite (class 1)\n- Graphene (class 2)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n\n# Define class names for the classifier\nclass_names = [\"diamond\", \"graphite\", \"graphene\"]\n\nstructures = []\n\n# Generate 10 diamond structures with small random perturbations\nfor i in range(10):\n    diamond = bulk(\"C\", \"diamond\", a=3.57)\n    diamond = diamond * (2, 2, 2)  # Make it bigger\n    diamond.rattle(stdev=0.5, seed=i)  # Add random perturbations\n    # Store string label in info, then convert to one-hot\n    diamond.info[\"class_label\"] = \"diamond\"\n    diamond.info[\"class\"] = class_to_onehot(\"diamond\", class_names)\n    structures.append(diamond)\n\n# Generate 10 graphite structures (using layered graphene-like structures)\nfor i in range(10):\n    # Create a graphite-like structure\n    graphite = graphene(formula=\"C2\", size=(3, 3, 1), a=2.46, vacuum=None)\n    # Stack two layers\n    layer2 = graphite.copy()\n    layer2.translate([0, 0, 3.35])\n    graphite.extend(layer2)\n    graphite.set_cell([graphite.cell[0], graphite.cell[1], [0, 0, 6.7]])\n    graphite.rattle(stdev=0.5, seed=i)\n    # Store string label in info, then convert to one-hot\n    graphite.info[\"class_label\"] = \"graphite\"\n    graphite.info[\"class\"] = class_to_onehot(\"graphite\", class_names)\n    structures.append(graphite)\n\n# Generate 10 graphene structures (single layer)\nfor i in range(10):\n    graphene_struct = graphene(formula=\"C2\", size=(3, 3, 1), a=2.46, vacuum=10.0)\n    graphene_struct.rattle(stdev=0.5, seed=i)\n    # Store string label in info, then convert to one-hot\n    graphene_struct.info[\"class_label\"] = \"graphene\"\n    graphene_struct.info[\"class\"] = class_to_onehot(\"graphene\", class_names)\n    structures.append(graphene_struct)\n\n# Save the structures to a file (these will be used for training)\nase.io.write(\"carbon_allotropes.xyz\", structures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting a pre-trained universal model\n\nHere, we download a pre-trained model checkpoint that will serve as the backbone\nfor our classifier. We will use PET-MAD, a universal interatomic potential for\nmaterials and molecules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "PET_MAD_URL = (\n    \"https://huggingface.co/lab-cosmo/pet-mad/resolve/v1.0.2/models/pet-mad-v1.0.2.ckpt\"\n)\n\nsubprocess.run([\"wget\", PET_MAD_URL], check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Classifier\n\nNow we can train the classifier. The classifier will learn to use the features learned\nby PET-MAD to classify our carbon allotropes.\n\nThe key hyperparameters are:\n\n- ``hidden_sizes``: The dimensions of the MLP layers. The last dimension (2 in this\n  case) acts as a bottleneck that can be used to extract collective variables. If\n  collective variables are not needed, this should be set to a larger value.\n- ``model_checkpoint``: Path to the pre-trained model (here PET-MAD).\n\n.. literalinclude:: options-classifier.yaml\n   :language: yaml\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here, we run training as a subprocess, in reality you would run this from the command\n# line as ``mtt train options-classifier.yaml -o classifier.pt``.\nsubprocess.run(\n    [\"mtt\", \"train\", \"options-classifier.yaml\", \"-o\", \"classifier.pt\"],\n    check=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the Trained Classifier\n\nOnce the classifier is trained, we can use it to predict class labels for new\nstructures or to extract bottleneck features (collective variables).\n\nLet's test the classifier on some structures:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the model\ncalc = MetatomicCalculator(\"classifier.pt\")\n\nstructures = ase.io.read(\"carbon_allotropes.xyz\", index=\":\")\n\n# Get predictions and compute per-class accuracy\nclass_names = [\"Diamond\", \"Graphite\", \"Graphene\"]\ncorrect_per_class = {0: 0, 1: 0, 2: 0}\ntotal_per_class = {0: 0, 1: 0, 2: 0}\n\nfor structure in structures:\n    probabilities = (\n        calc.run_model(\n            structure,\n            {\"mtt::class_probabilities\": ModelOutput(per_atom=False)},\n        )[\"mtt::class_probabilities\"]\n        .block()\n        .values.cpu()\n        .squeeze(0)\n        .numpy()\n    )\n    predicted_class = np.argmax(probabilities)\n    # Get actual class from one-hot encoding\n    actual_class = np.argmax(structure.info[\"class\"])\n    total_per_class[actual_class] += 1\n    if predicted_class == actual_class:\n        correct_per_class[actual_class] += 1\n\n# Compute accuracy for each class\naccuracies = [\n    correct_per_class[i] / total_per_class[i] * 100 if total_per_class[i] > 0 else 0\n    for i in range(3)\n]\n\n# Create a bar plot showing per-class accuracy\nplt.figure(figsize=(5, 3))\nbars = plt.bar(class_names, accuracies, color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Classifier Accuracy per Class\")\nplt.ylim(0, 105)\n\n# Add value labels in the middle of bars\nfor bar, acc in zip(bars, accuracies, strict=True):\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        bar.get_height() / 2,\n        f\"{acc:.0f}%\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=10,\n    )\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we extract the features learned by the classifier in our \"bottleneck\" layer.\nHaving only 2 dimensions allows us to easily visualize them. A low dimensionality\nis also necessary if we want to use these features as collective variables in enhanced\nsampling simulations.\nBy default the last layer before the output is used as bottleneck, but this can be\nconfigured with the model hyperparameter ``feature_layer_index``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Extract features\nbottleneck_features = []\nlabels = []\nprobabilities_list = []\nfor structure in structures:\n    features = (\n        calc.run_model(\n            structure,\n            {\"features\": ModelOutput(per_atom=False)},\n        )[\"features\"]\n        .block()\n        .values.cpu()\n        .squeeze(0)\n        .numpy()\n    )\n    probs = (\n        calc.run_model(\n            structure,\n            {\"mtt::class_probabilities\": ModelOutput(per_atom=False)},\n        )[\"mtt::class_probabilities\"]\n        .block()\n        .values.cpu()\n        .squeeze(0)\n        .numpy()\n    )\n    bottleneck_features.append(features)\n    # Get class from one-hot encoding\n    labels.append(np.argmax(structure.info[\"class\"]))\n    probabilities_list.append(probs)\nbottleneck_features = np.array(bottleneck_features)\nlabels = np.array(labels)\n\n# Plot the features for the three classes\nplt.figure(figsize=(5, 3))\nfor class_id in np.unique(labels):\n    mask = labels == class_id\n    if class_id == 0:\n        label = \"Diamond\"\n    elif class_id == 1:\n        label = \"Graphite\"\n    else:\n        label = \"Graphene\"\n    plt.scatter(\n        bottleneck_features[mask, 0],\n        bottleneck_features[mask, 1],\n        label=label,\n        alpha=0.3,\n    )\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"Features from Classifier\")\nplt.legend()\nplt.grid()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Visualization with Chemiscope\n\nWe can also create an interactive visualization using chemiscope, which allows\nus to explore the relationship between the structures and their bottleneck features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Prepare class labels as strings for visualization\nclass_names = [\"Diamond\", \"Graphite\", \"Graphene\"]\nclass_labels = [class_names[label] for label in labels]\n\n# Prepare probabilities for all classes\nprobabilities_array = np.array(probabilities_list, dtype=np.float64)\n\n# Create properties dictionary for chemiscope\nproperties = {\n    \"Feature 1\": bottleneck_features[:, 0],\n    \"Feature 2\": bottleneck_features[:, 1],\n    \"Class\": class_labels,\n    \"Probability Diamond\": probabilities_array[:, 0],\n    \"Probability Graphite\": probabilities_array[:, 1],\n    \"Probability Graphene\": probabilities_array[:, 2],\n}\n\n# Create the chemiscope visualization\nchemiscope.show(\n    structures,\n    properties=properties,\n    settings={\n        \"map\": {\n            \"x\": {\"property\": \"Feature 1\"},\n            \"y\": {\"property\": \"Feature 2\"},\n            \"color\": {\"property\": \"Class\"},\n        },\n        \"structure\": [{\"unitCell\": True}],\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the classifier model in PLUMED\n\nThe trained classifier model can also be used within PLUMED to define\ncollective variables based on the features learned by the classifier. Instructions\nfor using metatrain models with PLUMED can be found\n[here](https://www.plumed.org/doc-v2.10/user-doc/html/_m_e_t_a_o_m_i_c.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Fine-tune a pre-trained model\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This section of the documentation is only relevant for PET model so far.</p></div>\n\nThis section describes the process of fine-tuning a pre-trained model to\nadapt it to new tasks or datasets. Fine-tuning is a common technique used\nin machine learning, where a model is trained on a large dataset and then\nfine-tuned on a smaller dataset to improve its performance on specific tasks.\nSo far the fine-tuning capabilities are only available for PET model.\n\nThere is a complete example in `Fine-tune example <fine-tuning-example>`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Please note that the fine-tuning recommendations in this section are not universal\n  and require testing on your specific dataset to achieve the best results. You might\n  need to experiment with different fine-tuning strategies depending on your needs.</p></div>\n\n\n## Basic Fine-tuning\n\nThe basic way to fine-tune a model is to use the ``mtt train`` command with the\navailable pre-trained model defined in an ``options.yaml`` file. In this case, all the\nweights of the model will be adapted to the new dataset. In contrast to to the\ntraining continuation, the optimizer and scheduler state will be reset. You can still\nadjust the training hyperparameters in the ``options.yaml`` file, but the model\narchitecture will be taken from the checkpoint.\n\nTo set the path to the pre-trained model checkpoint, you need to specify the\n``read_from`` parameter in the ``options.yaml`` file:\n\n```yaml\narchitecture:\n  training:\n    finetune:\n      method: \"full\" # This stands for the full fine-tuning\n      read_from: path/to/checkpoint.ckpt\n```\nWe recommend to use a lower learning rate than the one used for the original training,\nas this will help stabilizing the training process. I.e. if the default learning rate is\n``1e-4``, you can set it to ``1e-5`` or even lower, using the following in the\n``options.yaml`` file:\n\n```yaml\narchitecture:\n  training:\n    learning_rate: 1e-5\n```\nPlease note, that in the case of the basic fine-tuning, the composition model weights\nwill be taken from the checkpoint and not adapted to the new dataset.\n\nThe basic fine-tuning strategy is a good choice in the case when the level of theory\nwhich is used for the original training is the same, or at least similar to the one used\nfor the new dataset. However, since this is not always the case, we also provide more\nadvanced fine-tuning strategies described below.\n\n\n## Fine-tuning model Heads\n\nAdapting all the model weights to a new dataset is not always the best approach. If the\nnew dataset consist of the same or similar data computed with a slightly different level\nof theory compared to the pre-trained models' dataset, you might want to keep the\nlearned representations of the crystal structures and only adapt the readout layers\n(i.e. the model heads) to the new dataset.\n\nIn this case, the ``mtt train`` command needs to be accompanied by the specific training\noptions in the ``options.yaml`` file. The following options need to be set:\n\n```yaml\narchitecture:\n  training:\n    finetune:\n      method: \"heads\"\n      read_from: path/to/checkpoint.ckpt\n      config:\n        head_modules: ['node_heads', 'edge_heads']\n        last_layer_modules: ['node_last_layers', 'edge_last_layers']\n```\nThe ``method`` parameter specifies the fine-tuning method to be used and the\n``read_from`` parameter specifies the path to the pre-trained model checkpoint. The\n``head_modules`` and ``last_layer_modules`` parameters specify the modules to be\nfine-tuned. Here, the ``node_*`` and ``edge_*`` modules represent different parts of the\nmodel readout layers related to the atom-based and bond-based features. The\n``*_last_layer`` modules are the last layers of the corresponding heads, implemented as\nmulti-layer perceptron (MLPs). You can select different combinations of the node and\nedge heads and last layers to be fine-tuned.\n\nWe recommend to first start the fine-tuning including all the modules listed above and\nexperiment with their different combinations if needed. You might also consider using a\nlower learning rate, e.g. ``1e-5`` or even lower, to stabilize the training process.\n\n\n## LoRA Fine-tuning\n\nIf the conceptually new type of structures is introduced in the new dataset, tuning only\nthe model heads might not be sufficient. In this case, you might need to adapt the\ninternal representations of the crystal structures. This can be done using the LoRA\ntechnique. However, in this case the model heads will be not adapted to the new dataset,\nso conceptually the level of theory should be consistent with the one used for the\npre-trained model.\n\n### What is LoRA?\n\nLoRA (Low-Rank Adaptation) stands for a Parameter-Efficient Fine-Tuning (PEFT)\ntechnique used to adapt pre-trained models to new tasks by introducing low-rank\nmatrices into the model's architecture.\n\nGiven a pre-trained model with the weights matrix $W_0$, LoRA introduces\nlow-rank matrices $A$ and $B$ of a rank $r$ such that the\nnew weights matrix $W$ is computed as:\n\n\\begin{align}W = W_0 + \\frac{\\alpha}{r} A B\\end{align}\n\nwhere $\\alpha$ is a regularization factor that controls the influence\nof the low-rank matrices on the model's weights. By adjusting the rank $r$\nand the regularization factor $\\alpha$, you can fine-tune the model\nto achieve better performance on specific tasks.\n\nTo use LoRA for fine-tuning, you need to provide the pre-trained model checkpoint with\nthe ``mtt train`` command and specify the LoRA parameters in the ``options.yaml`` file:\n\n```yaml\narchitecture:\n  training:\n    finetune:\n      method: \"lora\"\n      read_from: path/to/pre-trained-model.ckpt\n      config:\n        alpha: 0.1\n        rank: 4\n```\nThese parameters control the rank of the low-rank matrices introduced by LoRA\n(``rank``), and the regularization factor for the low-rank matrices (``alpha``).\nBy selecting the LoRA rank and the regularization factor, you can control the\namount of adaptation to the new dataset. Using lower values of the rank and\nthe regularization factor will lead to a more conservative adaptation, which can help\nbalancing the performance of the model on the original and new datasets.\n\nWe recommend to start with the LoRA parameters listed above and experiment with\ndifferent values if needed. You might also consider using a lower learning rate,\ne.g. ``1e-5`` or even lower, to stabilize the training process.\n\n\n## Fine-tuning on a new level of theory\n\nIf the new dataset is computed with a totally different level of theory compared to the\npre-trained model, which includes, for instance, the different composition energies, or\nyou want to fine-tune the model on a completely new target, you might need to consider\nthe transfer learning approach and introduce a new target in the ``options.yaml`` file.\nMore details about this approach can be found in the `Transfer Learning\n<transfer-learning>` section of the documentation.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
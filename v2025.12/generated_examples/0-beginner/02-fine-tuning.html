<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../genindex.html"><link rel="search" title="Search" href="../../search.html"><link rel="next" title="Training a model from scratch" href="03-train_from_scratch.html"><link rel="prev" title="How to prepare data for training" href="01-data_preparation.html">
        <link rel="canonical" href="https://docs.metatensor.org/metatrain/latest/generated_examples/0-beginner/02-fine-tuning.html">
        <link rel="prefetch" href="../../_static/images/metatrain-horizontal.png" as="image">
        <link rel="prefetch" href="../../_static/images/metatrain-horizontal-dark.png" as="image">

    <link rel="shortcut icon" href="../../_static/metatrain-64.png"><!-- Generated with Sphinx 8.2.3 and Furo 2025.09.25 -->
        <title>Fine-tune a pre-trained model - metatrain 2025.12 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/fontawesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/solid.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/brands.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles.css?v=e1ddca15" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">metatrain 2025.12 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../../_static/images/metatrain-horizontal.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../../_static/images/metatrain-horizontal-dark.png" alt="Dark Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../installation/installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../getting-started/index.html">Getting started</a><input aria-label="Toggle navigation of Getting started" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started/quickstart.html">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started/train_yaml_config.html">Training YAML Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started/override.html">Override Architecture’s Default Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started/checkpoints.html">Restarting and Checkpoints</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started/finetuning-example.html">Finetuning example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started/units.html">Units</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../architectures/index.html">Available Architectures</a><input aria-label="Toggle navigation of Available Architectures" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../architectures/generated/flashmd.html">FlashMD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../architectures/generated/gap.html">GAP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../architectures/generated/llpr.html">LLPR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../architectures/generated/nanopet.html">NanoPET (deprecated)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../architectures/generated/pet.html">PET</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../architectures/generated/soap_bpnn.html">SOAP-BPNN</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../index.html">Tutorials</a><input aria-label="Toggle navigation of Tutorials" checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">Beginner Tutorials</a><input aria-label="Toggle navigation of Beginner Tutorials" checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="00-basic-usage.html">Basic Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="01-data_preparation.html">How to prepare data for training</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">Fine-tune a pre-trained model</a></li>
<li class="toctree-l3"><a class="reference internal" href="03-train_from_scratch.html">Training a model from scratch</a></li>
<li class="toctree-l3"><a class="reference internal" href="04-parity_plot.html">Model validation with parity plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="05-run_ase.html">Running molecular dynamics with ASE</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../1-advanced/index.html">Advanced Tutorials</a><input aria-label="Toggle navigation of Advanced Tutorials" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/00-transfer-learning.html">Transfer Learning (experimental)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/01-llpr.html">Computing LLPR uncertainties</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/02-zbl.html">Training a model with ZBL corrections</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/03-fitting-generic-targets.html">Fitting generic targets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/04-flashmd.html">Training a FlashMD model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/05-multi-gpu.html">Multi-GPU training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/06-mixed-stress-training.html">Training with Mixed Stress Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/07-dos-training.html">Training a DOS model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../1-advanced/08-llpr-ensemble-training.html">Generating and training an LLPR-derived shallow ensemble model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../concepts/index.html">Concepts and Design</a><input aria-label="Toggle navigation of Concepts and Design" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/output-naming.html">Output naming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/loss-functions.html">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/auxiliary-outputs.html">Auxiliary outputs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cite.html">Citing Metatrain</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dev-docs/index.html">Developer documentation</a><input aria-label="Toggle navigation of Developer documentation" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dev-docs/getting-started.html">Contributing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev-docs/architecture-life-cycle.html">Life Cycle of an Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev-docs/new-architecture.html">Adding a new architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev-docs/dataset-information.html">Dataset Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev-docs/new-loss.html">Adding a new loss function</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dev-docs/cli/index.html">CLI API</a><input aria-label="Toggle navigation of CLI API" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/cli/train.html">Train</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/cli/eval.html">Eval</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/cli/export.html">Export</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/cli/formatter.html">Formatter</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dev-docs/utils/index.html">Utility API</a><input aria-label="Toggle navigation of Utility API" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../dev-docs/utils/additive/index.html">Additive models</a><input aria-label="Toggle navigation of Additive models" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/additive/remove_additive.html">Removing additive contributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/additive/composition.html">Composition model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/additive/zbl.html">ZBL short-range potential</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../dev-docs/utils/scaler/index.html">Scaler models</a><input aria-label="Toggle navigation of Scaler models" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/scaler/scaler.html">Scaler model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/scaler/remove_scale.html">Removing the scale from targets</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../dev-docs/utils/data/index.html">Data</a><input aria-label="Toggle navigation of Data" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/data/combine_dataloaders.html">Combining dataloaders</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/data/dataset.html">Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/data/get_dataset.html">Reading a dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/data/readers.html">Readers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/data/writers.html">Target data Writers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dev-docs/utils/data/systems_to_ase.html">Converting Systems to ASE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/abc.html">Abstract base classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/architectures.html">Architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/augmentation.html">Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/devices.html">Device</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/dtype.html">Dtype</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/errors.html">Errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/evaluate_model.html">Evaluating a model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/external_naming.html">External Naming</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/hypers.html">Hyperparameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/io.html">IO</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/logging.html">Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/long_range.html">Long-range</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/loss.html">Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/metrics.html">Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/neighbor_lists.html">Neighbor lists</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/omegaconf.html">Custom omegaconf functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/output_gradient.html">Output gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/per_atom.html">Averaging predictions per atom</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/pydantic.html">Pydantic utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/sum_over_atoms.html">Summing over atoms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/transfer.html">Data type and device transfers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dev-docs/utils/units.html">Unit handling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../dev-docs/base-hypers.html">Base hyperparameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev-docs/changelog.html">Changelog</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../../_sources/generated_examples/0-beginner/02-fine-tuning.rst.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-generated-examples-0-beginner-02-fine-tuning-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="fine-tune-a-pre-trained-model">
<span id="fine-tuning"></span><span id="sphx-glr-generated-examples-0-beginner-02-fine-tuning-py"></span><h1>Fine-tune a pre-trained model<a class="headerlink" href="#fine-tune-a-pre-trained-model" title="Link to this heading">¶</a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This section of the documentation is only relevant for PET model so far.</p>
</div>
<p>This section describes the process of fine-tuning a pre-trained model to
adapt it to new tasks or datasets. Fine-tuning is a common technique used
in machine learning, where a model is trained on a large dataset and then
fine-tuned on a smaller dataset to improve its performance on specific tasks.
So far the fine-tuning capabilities are only available for PET model.</p>
<p>There is a complete example in <a class="reference internal" href="../../getting-started/finetuning-example.html#fine-tuning-example"><span class="std std-ref">Fine-tune example</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please note that the fine-tuning recommendations in this section are not universal
and require testing on your specific dataset to achieve the best results. You might
need to experiment with different fine-tuning strategies depending on your needs.</p>
</div>
<section id="basic-fine-tuning">
<h2>Basic Fine-tuning<a class="headerlink" href="#basic-fine-tuning" title="Link to this heading">¶</a></h2>
<p>The basic way to fine-tune a model is to use the <code class="docutils literal notranslate"><span class="pre">mtt</span> <span class="pre">train</span></code> command with the
available pre-trained model defined in an <code class="docutils literal notranslate"><span class="pre">options.yaml</span></code> file. In this case, all the
weights of the model will be adapted to the new dataset. In contrast to to the
training continuation, the optimizer and scheduler state will be reset. You can still
adjust the training hyperparameters in the <code class="docutils literal notranslate"><span class="pre">options.yaml</span></code> file, but the model
architecture will be taken from the checkpoint.</p>
<p>To set the path to the pre-trained model checkpoint, you need to specify the
<code class="docutils literal notranslate"><span class="pre">read_from</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">options.yaml</span></code> file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">architecture</span><span class="p">:</span>
<span class="w">  </span><span class="nt">training</span><span class="p">:</span>
<span class="w">    </span><span class="nt">finetune</span><span class="p">:</span>
<span class="w">      </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;full&quot;</span><span class="w"> </span><span class="c1"># This stands for the full fine-tuning</span>
<span class="w">      </span><span class="nt">read_from</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">path/to/checkpoint.ckpt</span>
</pre></div>
</div>
<p>We recommend to use a lower learning rate than the one used for the original training,
as this will help stabilizing the training process. I.e. if the default learning rate is
<code class="docutils literal notranslate"><span class="pre">1e-4</span></code>, you can set it to <code class="docutils literal notranslate"><span class="pre">1e-5</span></code> or even lower, using the following in the
<code class="docutils literal notranslate"><span class="pre">options.yaml</span></code> file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">architecture</span><span class="p">:</span>
<span class="w">  </span><span class="nt">training</span><span class="p">:</span>
<span class="w">    </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-5</span>
</pre></div>
</div>
<p>Please note, that in the case of the basic fine-tuning, the composition model weights
will be taken from the checkpoint and not adapted to the new dataset.</p>
<p>The basic fine-tuning strategy is a good choice in the case when the level of theory
which is used for the original training is the same, or at least similar to the one used
for the new dataset. However, since this is not always the case, we also provide more
advanced fine-tuning strategies described below.</p>
<p>Here is the specification for the inputs to pass to the
<code class="docutils literal notranslate"><span class="pre">architecture.training.finetune</span></code> parameter in case of the basic fine-tuning:</p>
<dl class="py class">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.FullFinetuneHypers">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">metatrain.pet.modules.finetuning.</span></span><span class="sig-name descname"><span class="pre">FullFinetuneHypers</span></span><a class="reference internal" href="../../_modules/metatrain/pet/modules/finetuning.html#FullFinetuneHypers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers" title="Link to this definition">¶</a></dt>
<dd><p>Hyperparameters to use full finetuning of PET models.</p>
<p>This means all model parameters are trainable.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.FullFinetuneHypers.method">
<span class="sig-name descname"><span class="pre">method</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Literal" title="(in Python v3.14)"><span class="pre">Literal</span></a><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'full'</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'full'</span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers.method" title="Link to this definition">¶</a></dt>
<dd><p>Finetuning method to use.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.FullFinetuneHypers.read_from">
<span class="sig-name descname"><span class="pre">read_from</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers.read_from" title="Link to this definition">¶</a></dt>
<dd><p>Path to the pretrained model checkpoint.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.FullFinetuneHypers.config">
<span class="sig-name descname"><span class="pre">config</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.NotRequired" title="(in Python v3.14)"><span class="pre">NotRequired</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers.config" title="Link to this definition">¶</a></dt>
<dd><p>No configuration needed for full finetuning.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.FullFinetuneHypers.inherit_heads">
<span class="sig-name descname"><span class="pre">inherit_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers.inherit_heads" title="Link to this definition">¶</a></dt>
<dd><p>Mapping from new trainable targets (keys) to the existing targets
in the model (values).
This allows for copying weights from the corresponding
source heads to the destination heads instead of random initialization.</p>
</dd></dl>

</dd></dl>

</section>
<section id="fine-tuning-model-heads">
<h2>Fine-tuning model Heads<a class="headerlink" href="#fine-tuning-model-heads" title="Link to this heading">¶</a></h2>
<p>Adapting all the model weights to a new dataset is not always the best approach. If the
new dataset consist of the same or similar data computed with a slightly different level
of theory compared to the pre-trained models’ dataset, you might want to keep the
learned representations of the crystal structures and only adapt the readout layers
(i.e. the model heads) to the new dataset.</p>
<p>In this case, the <code class="docutils literal notranslate"><span class="pre">mtt</span> <span class="pre">train</span></code> command needs to be accompanied by the specific training
options in the <code class="docutils literal notranslate"><span class="pre">options.yaml</span></code> file. The following options need to be set:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">architecture</span><span class="p">:</span>
<span class="w">  </span><span class="nt">training</span><span class="p">:</span>
<span class="w">    </span><span class="nt">finetune</span><span class="p">:</span>
<span class="w">      </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;heads&quot;</span>
<span class="w">      </span><span class="nt">read_from</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">path/to/checkpoint.ckpt</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">head_modules</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&#39;node_heads&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;edge_heads&#39;</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">last_layer_modules</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&#39;node_last_layers&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;edge_last_layers&#39;</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">method</span></code> parameter specifies the fine-tuning method to be used and the
<code class="docutils literal notranslate"><span class="pre">read_from</span></code> parameter specifies the path to the pre-trained model checkpoint. The
<code class="docutils literal notranslate"><span class="pre">head_modules</span></code> and <code class="docutils literal notranslate"><span class="pre">last_layer_modules</span></code> parameters specify the modules to be
fine-tuned. Here, the <code class="docutils literal notranslate"><span class="pre">node_*</span></code> and <code class="docutils literal notranslate"><span class="pre">edge_*</span></code> modules represent different parts of the
model readout layers related to the atom-based and bond-based features. The
<code class="docutils literal notranslate"><span class="pre">*_last_layer</span></code> modules are the last layers of the corresponding heads, implemented as
multi-layer perceptron (MLPs). You can select different combinations of the node and
edge heads and last layers to be fine-tuned.</p>
<p>We recommend to first start the fine-tuning including all the modules listed above and
experiment with their different combinations if needed. You might also consider using a
lower learning rate, e.g. <code class="docutils literal notranslate"><span class="pre">1e-5</span></code> or even lower, to stabilize the training process.</p>
<p>Here is the specification for the inputs to pass to the
<code class="docutils literal notranslate"><span class="pre">architecture.training.finetune</span></code> parameter in case of <code class="docutils literal notranslate"><span class="pre">&quot;heads&quot;</span></code> fine-tuning:</p>
<dl class="py class">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.HeadsFinetuneHypers">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">metatrain.pet.modules.finetuning.</span></span><span class="sig-name descname"><span class="pre">HeadsFinetuneHypers</span></span><a class="reference internal" href="../../_modules/metatrain/pet/modules/finetuning.html#HeadsFinetuneHypers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers" title="Link to this definition">¶</a></dt>
<dd><p>Hyperparameters for heads finetuning of PET models.</p>
<p>Freezes all model parameters except for the prediction heads
and last layers.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.HeadsFinetuneHypers.method">
<span class="sig-name descname"><span class="pre">method</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Literal" title="(in Python v3.14)"><span class="pre">Literal</span></a><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'heads'</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'heads'</span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers.method" title="Link to this definition">¶</a></dt>
<dd><p>Finetuning method to use.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.HeadsFinetuneHypers.read_from">
<span class="sig-name descname"><span class="pre">read_from</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers.read_from" title="Link to this definition">¶</a></dt>
<dd><p>Path to the pretrained model checkpoint.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.HeadsFinetuneHypers.config">
<span class="sig-name descname"><span class="pre">config</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneConfig" title="metatrain.pet.modules.finetuning.HeadsFinetuneConfig"><span class="pre">HeadsFinetuneConfig</span></a></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers.config" title="Link to this definition">¶</a></dt>
<dd><p>Configuration for heads finetuning.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.HeadsFinetuneHypers.inherit_heads">
<span class="sig-name descname"><span class="pre">inherit_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers.inherit_heads" title="Link to this definition">¶</a></dt>
<dd><p>Mapping from new trainable targets (keys) to the existing targets
in the model (values).
This allows for copying weights from the corresponding
source heads to the destination heads instead of random initialization.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.HeadsFinetuneConfig">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">metatrain.pet.modules.finetuning.</span></span><span class="sig-name descname"><span class="pre">HeadsFinetuneConfig</span></span><a class="reference internal" href="../../_modules/metatrain/pet/modules/finetuning.html#HeadsFinetuneConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metatrain.pet.modules.finetuning.HeadsFinetuneConfig" title="Link to this definition">¶</a></dt>
<dd><p>Configuration for heads finetuning strategy.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.HeadsFinetuneConfig.head_modules">
<span class="sig-name descname"><span class="pre">head_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.HeadsFinetuneConfig.head_modules" title="Link to this definition">¶</a></dt>
<dd><p>List of module name prefixes for the prediction heads to finetune.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.HeadsFinetuneConfig.last_layer_modules">
<span class="sig-name descname"><span class="pre">last_layer_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.HeadsFinetuneConfig.last_layer_modules" title="Link to this definition">¶</a></dt>
<dd><p>List of module name prefixes for the last layers to finetune.</p>
</dd></dl>

</dd></dl>

</section>
<section id="lora-fine-tuning">
<h2>LoRA Fine-tuning<a class="headerlink" href="#lora-fine-tuning" title="Link to this heading">¶</a></h2>
<p>If the conceptually new type of structures is introduced in the new dataset, tuning only
the model heads might not be sufficient. In this case, you might need to adapt the
internal representations of the crystal structures. This can be done using the LoRA
technique. However, in this case the model heads will be not adapted to the new dataset,
so conceptually the level of theory should be consistent with the one used for the
pre-trained model.</p>
<section id="what-is-lora">
<h3>What is LoRA?<a class="headerlink" href="#what-is-lora" title="Link to this heading">¶</a></h3>
<p>LoRA (Low-Rank Adaptation) stands for a Parameter-Efficient Fine-Tuning (PEFT)
technique used to adapt pre-trained models to new tasks by introducing low-rank
matrices into the model’s architecture.</p>
<p>Given a pre-trained model with the weights matrix <span class="math notranslate nohighlight">\(W_0\)</span>, LoRA introduces
low-rank matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> of a rank <span class="math notranslate nohighlight">\(r\)</span> such that the
new weights matrix <span class="math notranslate nohighlight">\(W\)</span> is computed as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[W = W_0 + \frac{\alpha}{r} A B\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a regularization factor that controls the influence
of the low-rank matrices on the model’s weights. By adjusting the rank <span class="math notranslate nohighlight">\(r\)</span>
and the regularization factor <span class="math notranslate nohighlight">\(\alpha\)</span>, you can fine-tune the model
to achieve better performance on specific tasks.</p>
<p>To use LoRA for fine-tuning, you need to provide the pre-trained model checkpoint with
the <code class="docutils literal notranslate"><span class="pre">mtt</span> <span class="pre">train</span></code> command and specify the LoRA parameters in the <code class="docutils literal notranslate"><span class="pre">options.yaml</span></code> file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">architecture</span><span class="p">:</span>
<span class="w">  </span><span class="nt">training</span><span class="p">:</span>
<span class="w">    </span><span class="nt">finetune</span><span class="p">:</span>
<span class="w">      </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;lora&quot;</span>
<span class="w">      </span><span class="nt">read_from</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">path/to/pre-trained-model.ckpt</span>
<span class="w">      </span><span class="nt">config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="w">        </span><span class="nt">rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</pre></div>
</div>
<p>These parameters control the rank of the low-rank matrices introduced by LoRA
(<code class="docutils literal notranslate"><span class="pre">rank</span></code>), and the regularization factor for the low-rank matrices (<code class="docutils literal notranslate"><span class="pre">alpha</span></code>).
By selecting the LoRA rank and the regularization factor, you can control the
amount of adaptation to the new dataset. Using lower values of the rank and
the regularization factor will lead to a more conservative adaptation, which can help
balancing the performance of the model on the original and new datasets.</p>
<p>We recommend to start with the LoRA parameters listed above and experiment with
different values if needed. You might also consider using a lower learning rate,
e.g. <code class="docutils literal notranslate"><span class="pre">1e-5</span></code> or even lower, to stabilize the training process.</p>
<p>Here is the specification for the inputs to pass to the
<code class="docutils literal notranslate"><span class="pre">architecture.training.finetune</span></code> parameter in case of <code class="docutils literal notranslate"><span class="pre">&quot;lora&quot;</span></code> fine-tuning:</p>
<dl class="py class">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneHypers">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">metatrain.pet.modules.finetuning.</span></span><span class="sig-name descname"><span class="pre">LoRaFinetuneHypers</span></span><a class="reference internal" href="../../_modules/metatrain/pet/modules/finetuning.html#LoRaFinetuneHypers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers" title="Link to this definition">¶</a></dt>
<dd><p>Hyperparameters for LoRA finetuning of PET models.</p>
<p>Injects LoRA layers and finetunes only them.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneHypers.method">
<span class="sig-name descname"><span class="pre">method</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Literal" title="(in Python v3.14)"><span class="pre">Literal</span></a><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'lora'</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'lora'</span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers.method" title="Link to this definition">¶</a></dt>
<dd><p>Finetuning method to use</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneHypers.read_from">
<span class="sig-name descname"><span class="pre">read_from</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers.read_from" title="Link to this definition">¶</a></dt>
<dd><p>Path to the pretrained model checkpoint.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneHypers.config">
<span class="sig-name descname"><span class="pre">config</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig" title="metatrain.pet.modules.finetuning.LoRaFinetuneConfig"><span class="pre">LoRaFinetuneConfig</span></a></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers.config" title="Link to this definition">¶</a></dt>
<dd><p>Configuration for LoRA finetuning.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneHypers.inherit_heads">
<span class="sig-name descname"><span class="pre">inherit_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers.inherit_heads" title="Link to this definition">¶</a></dt>
<dd><p>Mapping from new trainable targets (keys) to the existing targets
in the model (values).
This allows for copying weights from the corresponding
source heads to the destination heads instead of random initialization.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneConfig">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">metatrain.pet.modules.finetuning.</span></span><span class="sig-name descname"><span class="pre">LoRaFinetuneConfig</span></span><a class="reference internal" href="../../_modules/metatrain/pet/modules/finetuning.html#LoRaFinetuneConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig" title="Link to this definition">¶</a></dt>
<dd><p>Configuration for LoRA finetuning strategy.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneConfig.rank">
<span class="sig-name descname"><span class="pre">rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig.rank" title="Link to this definition">¶</a></dt>
<dd><p>Rank of the LoRA matrices.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneConfig.alpha">
<span class="sig-name descname"><span class="pre">alpha</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig.alpha" title="Link to this definition">¶</a></dt>
<dd><p>Scaling factor for the LoRA matrices.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metatrain.pet.modules.finetuning.LoRaFinetuneConfig.target_modules">
<span class="sig-name descname"><span class="pre">target_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.NotRequired" title="(in Python v3.14)"><span class="pre">NotRequired</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig.target_modules" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="fine-tuning-on-a-new-level-of-theory">
<h2>Fine-tuning on a new level of theory<a class="headerlink" href="#fine-tuning-on-a-new-level-of-theory" title="Link to this heading">¶</a></h2>
<p>If the new dataset is computed with a totally different level of theory compared to the
pre-trained model, which includes, for instance, the different composition energies, or
you want to fine-tune the model on a completely new target, you might need to consider
the transfer learning approach and introduce a new target in the <code class="docutils literal notranslate"><span class="pre">options.yaml</span></code> file.
More details about this approach can be found in the <a class="reference internal" href="../1-advanced/00-transfer-learning.html#transfer-learning"><span class="std std-ref">Transfer Learning</span></a> section of the documentation.</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-generated-examples-0-beginner-02-fine-tuning-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/e823fee7ce26529b84163bf9cde51b8d/02-fine-tuning.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">02-fine-tuning.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/39253bead16e93ea8038f2e27ca46308/02-fine-tuning.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">02-fine-tuning.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/321ebbe462b58aa5826230f68e5cc9df/02-fine-tuning.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">02-fine-tuning.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="03-train_from_scratch.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Training a model from scratch</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="01-data_preparation.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">How to prepare data for training</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, metatrain developers
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link fa-brands fa-github fa-2x" href="https://github.com/metatensor/metatrain" aria-label="GitHub"></a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Fine-tune a pre-trained model</a><ul>
<li><a class="reference internal" href="#basic-fine-tuning">Basic Fine-tuning</a><ul>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers"><code class="docutils literal notranslate"><span class="pre">FullFinetuneHypers</span></code></a><ul>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers.method"><code class="docutils literal notranslate"><span class="pre">FullFinetuneHypers.method</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers.read_from"><code class="docutils literal notranslate"><span class="pre">FullFinetuneHypers.read_from</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers.config"><code class="docutils literal notranslate"><span class="pre">FullFinetuneHypers.config</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.FullFinetuneHypers.inherit_heads"><code class="docutils literal notranslate"><span class="pre">FullFinetuneHypers.inherit_heads</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#fine-tuning-model-heads">Fine-tuning model Heads</a><ul>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers"><code class="docutils literal notranslate"><span class="pre">HeadsFinetuneHypers</span></code></a><ul>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers.method"><code class="docutils literal notranslate"><span class="pre">HeadsFinetuneHypers.method</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers.read_from"><code class="docutils literal notranslate"><span class="pre">HeadsFinetuneHypers.read_from</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers.config"><code class="docutils literal notranslate"><span class="pre">HeadsFinetuneHypers.config</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneHypers.inherit_heads"><code class="docutils literal notranslate"><span class="pre">HeadsFinetuneHypers.inherit_heads</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneConfig"><code class="docutils literal notranslate"><span class="pre">HeadsFinetuneConfig</span></code></a><ul>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneConfig.head_modules"><code class="docutils literal notranslate"><span class="pre">HeadsFinetuneConfig.head_modules</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.HeadsFinetuneConfig.last_layer_modules"><code class="docutils literal notranslate"><span class="pre">HeadsFinetuneConfig.last_layer_modules</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#lora-fine-tuning">LoRA Fine-tuning</a><ul>
<li><a class="reference internal" href="#what-is-lora">What is LoRA?</a><ul>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneHypers</span></code></a><ul>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers.method"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneHypers.method</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers.read_from"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneHypers.read_from</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers.config"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneHypers.config</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneHypers.inherit_heads"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneHypers.inherit_heads</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneConfig</span></code></a><ul>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig.rank"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneConfig.rank</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig.alpha"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneConfig.alpha</span></code></a></li>
<li><a class="reference internal" href="#metatrain.pet.modules.finetuning.LoRaFinetuneConfig.target_modules"><code class="docutils literal notranslate"><span class="pre">LoRaFinetuneConfig.target_modules</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#fine-tuning-on-a-new-level-of-theory">Fine-tuning on a new level of theory</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=50a9be24"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/toggleprompt.js?v=5801b3bb"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>
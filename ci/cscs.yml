include:
  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'

stages:
  - build_base
  - build
  - test

# Build base image from Containerfile.base
# The base image is re-built whenever Containerfile.base changes
build base:
  stage: build_base
  extends: .container-builder-cscs-gh200
  before_script:
    - TAG_DOCKERFILE=`sha256sum $DOCKERFILE | head -c 8`
    - TAG=${TAG_DOCKERFILE}
    - export PERSIST_IMAGE_NAME=$CSCS_REGISTRY_PATH/base/mtt-base:$TAG
    - echo "BASE_IMAGE=$PERSIST_IMAGE_NAME" > build.env
    - 'echo "INFO: Building image $PERSIST_IMAGE_NAME"'
  artifacts:
    reports:
      dotenv: build.env
  variables:
    DOCKERFILE: ci/containers/Containerfile.base

# Build image with metatrain
build mtt:
  stage: build
  extends: .container-builder-cscs-gh200
  before_script:
    - 'echo "INFO: Building image $PERSIST_IMAGE_NAME"'
  variables:
    GIT_STRATEGY: clone
    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/base/mtt:$CI_COMMIT_SHORT_SHA
    DOCKERFILE: ci/containers/Containerfile.mtt
    DOCKER_BUILD_ARGS: '["BASE_IMAGE"]'

# Run tox tests (single GPU)
test tox:
  stage: test
  # Trigger tox tests as soon as base image is ready
  needs:
    - build base
  extends: .container-runner-daint-gh200
  image: $BASE_IMAGE
  timeout: 24h
  script:
    - rm -rf /tmp/tox/
    - tox -e check-cuda-available --workdir /tmp/tox/ --list-dependencies
    - tox --workdir /tmp/tox/ --list-dependencies
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_PARTITION: normal
    SLURM_NTASKS: 1
    SLURM_TIMELIMIT: '00:30:00'
    GIT_STRATEGY: fetch

# Run distributed training (single node, multiple GPUs)
test distributed:
  stage: test
  extends: .container-runner-daint-gh200
  image: $CSCS_REGISTRY_PATH/base/mtt:$CI_COMMIT_SHORT_SHA
  timeout: 24h
  script:
    - . /mtt-venv/bin/activate
    - cd tests/distributed
    - mtt train options-distributed.yaml | tee train.log
    - 'if [ $SLURM_PROCID == 0 ]; then grep "Training on 4 devices" train.log || { echo "FAILED: Not training on 4 devices as requested."; exit 1; }; fi'
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_PARTITION: normal
    SLURM_NTASKS: 4
    SLURM_GPUS_PER_TASK: 1
    SLURM_CPUS_PER_TASK: 16
    SLURM_TIMELIMIT: '00:10:00'
    SLURM_UNBUFFEREDIO: 1
    USE_NCCL: 'cuda12'
    GIT_STRATEGY: fetch

test distributed multitarget:
  stage: test
  extends: .container-runner-daint-gh200
  image: $CSCS_REGISTRY_PATH/base/mtt:$CI_COMMIT_SHORT_SHA
  timeout: 24h
  script:
    - . /mtt-venv/bin/activate
    - cd tests/distributed
    - mtt train options-distributed-multitarget.yaml -o model-multitarget.pt | tee train-multitarget.log
    - 'if [ $SLURM_PROCID == 0 ]; then grep "Training on 4 devices" train-multitarget.log || { echo "FAILED: Not training on 4 devices as requested."; exit 1; }; fi'
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_PARTITION: normal
    SLURM_NTASKS: 4
    SLURM_GPUS_PER_TASK: 1
    SLURM_CPUS_PER_TASK: 16
    SLURM_TIMELIMIT: '00:10:00'
    SLURM_UNBUFFEREDIO: 1
    USE_NCCL: 'cuda12'
    GIT_STRATEGY: fetch
  artifacts:
    paths:
      - tests/distributed/model-multitarget.pt
      - tests/distributed/model-multitarget.ckpt

test distributed multitarget finetuning:
  stage: test
  needs:
    - job: "test distributed multitarget"
      artifacts: true
  extends: .container-runner-daint-gh200
  image: $CSCS_REGISTRY_PATH/base/mtt:$CI_COMMIT_SHORT_SHA
  timeout: 24h
  script:
    - . /mtt-venv/bin/activate
    - cd tests/distributed
    - mtt train options-distributed-finetuning.yaml --restart model-multitarget.ckpt | tee train-multitarget-finetuning.log
    - 'if [ $SLURM_PROCID == 0 ]; then grep "Training on 4 devices" train-multitarget-finetuning.log || { echo "FAILED: Not training on 4 devices as requested."; exit 1; }; fi'
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_PARTITION: normal
    SLURM_NTASKS: 4
    SLURM_GPUS_PER_TASK: 1
    SLURM_CPUS_PER_TASK: 16
    SLURM_TIMELIMIT: '00:10:00'
    SLURM_UNBUFFEREDIO: 1
    USE_NCCL: 'cuda12'
    GIT_STRATEGY: fetch

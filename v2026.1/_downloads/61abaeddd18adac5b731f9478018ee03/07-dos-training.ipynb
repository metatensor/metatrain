{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a DOS model\n\nThis tutorial demonstrates how to train a model for the prediction\nof the electronic density of states (DOS), while accounting for the unique properties\nof the DOS using the `masked dos loss function <dos-loss>`. This procedure can be\nused to train PET-MAD-DOS, a universal model for the electronic density\nof states. (https://arxiv.org/abs/2508.17418)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import subprocess\n\nimport ase\nimport ase.io\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n\nIn order to use the masked dos loss function, we need to extract the DOS,\nthe mask, and prepare them in a way to facilitate the use of extra targets\nduring training. The extra targets parameter gives the model freedom to shift\nthe energy reference of the target DOS, which is important as the energy\nreference is ill-defined for bulk systems. In this example, we will demonstrate\nthe entire data processing pipeline, using 200 extra targets, starting from the\neigenvalues and k-point weights obtained from a DFT calculation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_extra_targets = 200\nstructures = ase.io.read(\"DOS_structures.xyz\", \":\")\n# Each structure contains the eigenvalues and k-point weights as arrays in info.\neigenvalues = []\nkweights = []\nfor structure_i in structures:\n    eigenvalues.append(structure_i.info[\"eigenvalues\"])  # shape (n_kpoints, n_bands)\n    kweights.append(\n        structure_i.info[\"k-point weights\"]\n    )  # shape (n_kpoints,), sums to 2 in this example\n\nfor index, eigenvalue_i in enumerate(eigenvalues):\n    print(\"Range of structure \", index)\n    print(np.min(eigenvalue_i), np.max(eigenvalue_i))\n\n# Here we can see that the eigenvalue range for both structures differ\n# very significantly from one another which complicates training, which\n# is why we introduce the mask to focus the loss on the relevant energy\n# ranges for each structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Processing\n\nNext, we need to process the eigenvalues and k-point weights to obtain\nthe DOS and the mask. We will use a Gaussian smearing to compute the\nDOS on a fixed energy grid.# The mask will be defined as 1 in the energy\nrange where we are confident that all the relevant eigenvalues have been\ncomputed, and 0 elsewhere. In this example, we define the upper bound of\nthe confident energy range as 0.9eV below the minimum eigenvalue of the\nhighest energy band in the structure. First, let us define the fixed\nenergy grid and pick a smearing width of 0.3eV.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smearing = 0.3\nall_eigenvalues = np.concatenate([i.flatten() for i in eigenvalues])\ne_min = np.min(all_eigenvalues) - 1.5\ne_max = np.max(all_eigenvalues) + 1.5\n\nenergy_grid = np.arange(e_min, e_max, 0.05)\n\n# Here we chose to use a grid of 0.05eV spacing and we pick the energy\n# grid to extend 1.5eV, representing 5 standard deviations, beyond the\n# min and max eigenvalues across all structures.\n\n# After defining the energy grid, we can now compute the DOS and the\n# mask for each structure.\nfor index in range(len(structures)):\n    confident_energy_upper_bound = (\n        np.min(eigenvalues[index][:, -1]) - 0.9\n    )  # 0.9eV below the minimum of the highest band\n    normalization = 1 / np.sqrt(\n        2 * np.pi * smearing**2\n    )  # Gaussian normalization factor\n    eigenvalues_i = eigenvalues[index].flatten()\n    # Flatten eigenvalues to shape (n_kpoints * n_bands,)\n    kweights_i = kweights[index].repeat(eigenvalues[index].shape[1])\n    # Ensure that the eigenvalues are mapped to the correct weight,\n    # shape (n_kpoints * n_bands,)\n\n    dos_i = (\n        np.sum(\n            kweights_i[:, None]\n            * (np.exp(-0.5 * ((energy_grid - eigenvalues_i[:, None]) / smearing) ** 2)),\n            axis=0,\n        )\n        * normalization\n    )  # Apply Gaussian smearing and sum contributions\n    mask_i = (energy_grid <= confident_energy_upper_bound).astype(\n        int\n    )  # Define the mask\n\n    # padding the dos and mask with zeros in front based on extra targets,\n    # these values will be ignored during loss computation\n    dos_i_padded = np.concatenate([np.zeros(n_extra_targets), dos_i])\n    mask_i_padded = np.concatenate([np.zeros(n_extra_targets), mask_i])\n\n    # Store the dos and mask in the structure info for later use during training\n    structures[index].info[\"dos\"] = dos_i_padded.astype(np.float32)\n    structures[index].info[\"dos_mask\"] = mask_i_padded.astype(np.float32)\n\n\n# Write the structures to an xyz file\nase.io.write(\"DOS.xyz\", structures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n\nThe dataset is now ready for training. You can now provide it to ``metatrain`` and\ntrain your DOS model!\n\nFor example, you can use the following options file:\n\n.. literalinclude:: options-dos.yaml\n   :language: yaml\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We disable composition contributions because it is difficult to fit the DOS\n# using a composition model, accounting for the ill-defined energy reference of\n# the DOS. ``scale_targets`` is also set to false because it does not support masks.\n# For details regarding the parameters of the loss function, please refer\n# to the :ref:`masked dos loss function <dos-loss>` documentation. Additionally,\n# the mask should be provided as extra data and share the same name as the target\n# DOS with a \"_mask\" suffix. Due to the small dataset in this example, we set the\n# validation set to be identical to the train set. In practice, you should use a\n# separate validation set or set it as a fraction of the training set.\n\n# Here, we run training as a subprocess, in reality you\n# would run this from the command line as ``mtt train options-dos.yaml``.\nsubprocess.run([\"mtt\", \"train\", \"options-dos.yaml\"], check=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
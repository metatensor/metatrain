{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fine-tuning a pre-trained model\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Finetuning is currently only available for the PET architecture.</p></div>\n\n\nThis is a simple example for fine-tuning PET-MAD (or a general PET model), that\ncan be used as a template for general fine-tuning with metatrain.\nFine-tuning a pretrained model allows you to obtain a model better suited for\nyour specific system. You need to provide a dataset of structures that have\nbeen evaluated at a higher reference level of theory, usually DFT. Fine-tuning\na universal model such as PET-MAD allows for reasonable model performance even if little\ntraining data is available.\nIt requires using a pre-trained model checkpoint with the ``mtt train`` command and\nsetting the new targets corresponding to the new level of theory in the ``options.yaml``\nfile.\n\n\n\nWe start by setting up the ``options.yaml`` file. Here we specify to fine-tune on a\nsmall model dataset containing structures of ethanol, labelled with energies and\nforces. We can specify the fine-tuning method in the ``finetune`` block in the\n``training`` options of the ``architecture``. Here, the basic ``full`` option is\nchosen, which finetunes all weights of the model. All available fine-tuning methods\nare found in the concepts page `Fine-tuning <label_fine_tuning_concept>`. This\nsection discusses implementation details, options and recommended use cases. Other\nfine-tuning options can be simply substituted in this script, by changing the\n``finetune`` block.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Since our dataset has energies and forces obtained from reference calculations,\n  different from the reference of the pretrained model, it is recommended to create a\n  new energy head. Using this so-called energy variant can be simply invoked by\n  requesting a new target in the options file. Follow the nomenclature\n  ``energy/{yourname}``.</p></div>\n\n\nFurthermore, you need to specify the checkpoint, that you want to fine-tune in\nthe ``read_from`` option.\n\nA simple ``options-ft.yaml`` file for this task could look like this:\n\n.. literalinclude:: options-ft.yaml\n  :language: yaml\n\nIn this example, we specified a low number of :attr:`num_epochs` and a relatively high\n:attr:`learning_rate`, for short compilation time. Usually, the ``learning_rate`` is\nchosen to be relatively low. Typically lower, than the ``learning_rate`` that the model\nhas been per-trained on.\nto stabilise training.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Note that in ``targets`` we use the ``energy/finetune`` head, differing from the\n  default ``energy`` head. This means, that the model creates a new head with a new\n  composition model for the new reference energies provided in your dataset. While\n  the old energy reference is still available, it is rendered useless, as we trained\n  all weights of the model. If you want to obtain a model with multiple energy heads,\n  you can simply train on multiple energy references simultaneously. This and other\n  more advanced fine-tuning strategies are discussed in\n  `Fine-tuning concepts <label_fine_tuning_concept>`.</p></div>\n\n\nWe assumed that the pre-trained model is trained on the dataset\n``ethanol_reduced_100.xyz`` in which energies are written in the ``energy`` key of\nthe ``info`` dictionary of the dataset.\nAdditionally, forces should be provided with corresponding keys\nwhich you can specify in the ``options-ft.yaml`` file under ``targets``.\nFurther information on specifying targets can be found in the `data section of\nthe Training YAML Reference <data-section>`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>It is important that the ``length_unit`` is set to ``angstrom`` and the ``energy``\n  ``unit`` is ``eV`` in order to match the units of your reference data.</p></div>\n\n\nAfter setting up your ``options-ft.yaml`` file, you can then simply run:\n\n```bash\nmtt train options-ft.yaml -o model-ft.pt\n```\nYou can check finetuning training curves by parsing the ``train.csv`` that is written\nby ``mtt train``. We remove the old outputs folder from other examples, which\nis not necessary for the normal usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import glob\nimport subprocess\n\nimport ase.io\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom metatomic.torch.ase_calculator import MetatomicCalculator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In order to obtain a pretrained model, you can use a PET-MAD checkpoint from\n# huggingface. Here, we get the PET-MAD ckpt, run ``mtt train`` as a subprocess, and\n# delete the old outputs folder and old model checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run([\"rm\", \"-rf\", \"outputs\"], check=True)\nsubprocess.run(\n    [\n        \"wget\",\n        \"https://huggingface.co/lab-cosmo/pet-mad/resolve/v1.1.0/models/pet-mad-v1.1.0.ckpt\",\n    ],\n    check=True,\n)\n\nsubprocess.run([\"mtt\", \"train\", \"options-ft.yaml\", \"-o\", \"model-ft.pt\"], check=True)\nsubprocess.run([\"rm\", \"-rf\", \"pet-mad-v1.1.0.ckpt\"], check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training, we can check if finetuning was successful.\nFirst we check the training curves, that are saved in ``train.csv`` in the outputs\nfolder. We start with parsing the csv file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "csv_path = glob.glob(\"outputs/*/*/train.csv\")[-1]\nwith open(csv_path, \"r\") as f:\n    header = f.readline().strip().split(\",\")\n    f.readline()  # skip units row\n\n# Build dtype\ndtype = [(h, float) for h in header]\n\n# Load data as plain float array\ndata = np.loadtxt(csv_path, delimiter=\",\", skiprows=2)\n\n# Convert to structured\nstructured = np.zeros(data.shape[0], dtype=dtype)\nfor i, h in enumerate(header):\n    structured[h] = data[:, i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's plot the learning curves.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_energy_RMSE = structured[\"training energy/finetune RMSE (per atom)\"]\ntraining_forces_MAE = structured[\"training forces[energy/finetune] MAE\"]\nvalidation_energy_RMSE = structured[\"validation energy/finetune RMSE (per atom)\"]\nvalidation_forces_MAE = structured[\"validation forces[energy/finetune] MAE\"]\n\nfig, axs = plt.subplots(1, 2, figsize=((8, 3.5)))\n\naxs[0].plot(training_energy_RMSE, label=\"training energy RMSE\")\naxs[0].plot(validation_energy_RMSE, label=\"validation energy RMSE\")\naxs[0].set_xlabel(\"Epochs\")\naxs[0].set_ylabel(\"energy / meV\")\naxs[0].set_xscale(\"log\")\naxs[0].set_yscale(\"log\")\naxs[0].legend()\naxs[1].plot(training_forces_MAE, label=\"training forces MAE\")\naxs[1].plot(validation_forces_MAE, label=\"validation forces MAE\")\naxs[1].set_ylabel(\"force / meV/A\")\naxs[1].set_xlabel(\"Epochs\")\naxs[1].set_xscale(\"log\")\naxs[1].set_yscale(\"log\")\naxs[1].legend()\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that the validation loss still decreases, however, for the sake of brevity\nof this exercise we only finetuned for a few epochs. As further check for how well\nyour fine-tuned model performs on a dataset of choice, we can check the parity plots\nfor energy and force\n(see `sphx_glr_generated_examples_0-beginner_04-parity_plot.py`).\nFor evaluation, we can compare performance of our fine-tuned model and the base model\nPET-MAD. Using ``mtt eval`` we can simply evaluate our new energy head, by specifying\nit in the options-ft-eval.yaml:\n\n```yaml\nsystems: ethanol_reduced_100.xyz\ntargets:\n  energy/finetune:\n    key: energy\n    unit: eV\n    forces:\n      key: forces\n```\nand then run\n\n```bash\nmtt eval model-ft.pt options-ft-eval.yaml -o output-ft.xyz\n```\nThen you can simply read the predicted energies in the headers of the xyz file.\nAnother possibility is to load your fine-tuned model ``model-ft.pt`` as ``metatomic``\nmodel and evaluate energies and forces with ASE in Python.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_capture_repr_block = ()\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.seterr()\ntargets = ase.io.read(\n    \"ethanol_reduced_100.xyz\",\n    format=\"extxyz\",\n    index=\":\",\n)\ncalc_ft = MetatomicCalculator(\n    \"model-ft.pt\", variants={\"energy\": \"finetune\"}, extensions_directory=None\n)\n# specify variant suffix here\nwith np.errstate(invalid=\"ignore\"):\n    e_targets = np.array(\n        [frame.get_total_energy() / len(frame) for frame in targets]\n    )  # target energies\n    f_targets = np.array(\n        [frame.get_forces().flatten() for frame in targets]\n    ).flatten()  # target forces\n\n    for frame in targets:\n        frame.calc = calc_ft\n\n    e_predictions = np.array(\n        [frame.get_total_energy() / len(frame) for frame in targets]\n    )  # predicted energies\n    f_predictions = np.array(\n        [frame.get_forces().flatten() for frame in targets]\n    ).flatten()  # predicted forces\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\n# Parity plot for energies\naxs[0].scatter(e_targets, e_predictions, label=\"FT\")\naxs[0].axline((np.min(e_targets), np.min(e_targets)), slope=1, ls=\"--\", color=\"red\")\naxs[0].set_xlabel(\"Target energy / meV\")\naxs[0].set_ylabel(\"Predicted energy / meV\")\nmin_e = np.min(np.array([e_targets, e_predictions])) - 2\nmax_e = np.max(np.array([e_targets, e_predictions])) + 2\naxs[0].set_title(\"Energy Parity Plot\")\naxs[0].set_xlim(min_e, max_e)\naxs[0].set_ylim(min_e, max_e)\n\n# Parity plot for forces\naxs[1].scatter(f_targets, f_predictions, alpha=0.5, label=\"FT\")\naxs[1].axline((np.min(f_targets), np.min(f_targets)), slope=1, ls=\"--\", color=\"red\")\naxs[1].set_xlabel(\"Target force / meV/\u00c5\")\naxs[1].set_ylabel(\"Predicted force / meV/\u00c5\")\nmin_f = np.min(np.array([f_targets, f_predictions])) - 2\nmax_f = np.max(np.array([f_targets, f_predictions])) + 2\naxs[1].set_title(\"Force Parity Plot\")\naxs[1].set_xlim(min_f, max_f)\naxs[1].set_ylim(min_f, max_f)\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the fine-tuning gives reasonable predictions on energies and forces.\nSince the training was limited to 10 epochs in this example, the results can be\nobviously improved by training for more epochs and optimizing training\nhyperparameters.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>To learn about more elaborate fine-tuning strategies and tools, check out the\n  fine-tuning examples in the\n  [AtomisticCookbook](https://atomistic-cookbook.org/examples/pet-finetuning/pet-ft.html)</p></div>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
# These hypers are simply kept in sync with the defaults in MACE,
# except for some metatrain specific hypers.
# They are autogenerated by _gen_hypers.py
architecture:
  name: experimental.mace
  model:
    cutoff: 5.0  # Named "r_max" in MACE
    num_radial_basis: 8
    num_cutoff_basis: 5
    max_ell: 3
    interaction: RealAgnosticResidualInteractionBlock
    num_interactions: 2
    hidden_irreps: None
    edge_irreps: None
    apply_cutoff: true
    avg_num_neighbors: 1
    pair_repulsion: false
    distance_transform: None
    correlation: 3
    gate: None
    interaction_first: RealAgnosticResidualInteractionBlock
    MLP_irreps: 16x0e
    radial_MLP: [64, 64, 64]
    radial_type: bessel
    use_embedding_readout: false
    use_last_readout_only: false
    use_agnostic_product: false
  training:
    # Optimizer hypers (directly using MACE's scripts)
    optimizer: adam
    learning_rate: 0.01  # Named "lr" in MACE
    weight_decay: 5e-07
    amsgrad: true
    beta: 0.9
    # Scheduler hypers (directly using MACE's scripts)
    lr_scheduler: ReduceLROnPlateau  # Named "scheduler" in MACE
    lr_scheduler_gamma: 0.9993
    lr_factor: 0.8
    lr_scheduler_patience: 50  # Named "scheduler_patience" in MACE
    # General training parameters that are shared across architectures
    distributed: false
    distributed_port: 39591
    batch_size: 16
    num_epochs: 1000
    log_interval: 1
    checkpoint_interval: 100
    scale_targets: true
    fixed_composition_weights: {}
    fixed_scaling_weights: {}
    per_structure_targets: []
    num_workers: null
    log_mae: true
    log_separate_blocks: false
    best_model_metric: mae_prod
    grad_clip_norm: 1.0
    loss: mse

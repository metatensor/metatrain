{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-GPU training\n\n``metatrain`` supports training a model with several GPUs, which can accelerate the\ntraining, especially when the training dataset is large / there are many training\nepochs. This feature is enabled by the :py:mod:`torch.distributed` module, and thus can\ndo multiprocess parallelism across several nodes.\n\nIn multi-GPU training, every batch of samples is split into smaller mini-batches and the\ncomputation is run for each of the smaller mini-batches in parallel on different GPUs.\nThe different gradients obtained on each device are then summed. This approach allows\nthe user to reduce the time it takes to train models.\n\nTo know if the model supports multi-GPU training, please check [Available Architectures](../../docs/src/architectures/index.rst) and see if the default hyperparameters have\nthe ``distributed`` option.\n\n## Input file\nTo do this, you only need to switch on the ``distributed`` option in the ``.yaml`` file\nfor the training. Let's take [this tutorial](../beginner_tutorials/train-from-scratch.rst) as an example. Now, the\n``options.yaml`` is\n\n.. literalinclude:: ./options-distributed.yaml\n   :language: yaml\n   :linenos:\n\n## Slurm script\n\nBelow is an example Slurm script for submitting the job. Please be aware that the actual\nconfigurations vary from clusters to clusters, so you have to modify it. Different\nscheduler will require similar options. ``metatrain`` will automatically use all the\nGPUs that you have asked for. You should make a single GPU visible for each process\n(setting `--gpus-per-node` equal to the number of GPUs, or setting `--gpus-per-task=1`,\ndepending on your cluster configuration).\n\n```bash\n#!/bin/bash\n#SBATCH --nodes 1\n#SBATCH --ntasks 2  # must equal to the number of GPUs\n#SBATCH --ntasks-per-node 2\n#SBATCH --gpus-per-node 2  # use 2 GPUs\n#SBATCH --cpus-per-task 8\n#SBATCH --exclusive\n#SBATCH --partition=h100  # adapt this to your cluster\n#SBATCH --time=1:00:00\n\n# load modules and/or virtual environments and/or containers here\n\nsrun mtt train options-distributed.yaml\n```\n## Performance\n\nIf the multi-GPU training runs successfully, you should see this in the training log:\n\n```bash\n[2025-10-08 11:34:22][INFO] - Distributed environment set up with MASTER_ADDR=kh080,\nMASTER_PORT=39591, WORLD_SIZE=2, RANK=0, LOCAL_RANK=0\n[2025-10-08 11:34:23][INFO] - Training on 2 devices with dtype torch.float32\n```\nThis 100-epoch training takes 23 seconds.\n\n```bash\n[2025-10-08 11:34:22][INFO] - Starting training from scratch\n...\n[2025-10-08 11:34:45][INFO] - Training finished!\n```\nNow let's switch off the multi-GPU training by writing ``distributed: false``, and\nsubmit this job again. The training takes 69 seconds.\n\n```bash\n[2025-10-08 11:37:38][INFO] - Setting up model\n...\n[2025-10-08 11:38:47][INFO] - Training finished!\n```\n## Multi-GPU fine-tuning\nYou can use multi-GPU for fine-tuning too, by writing ``distributed: True`` in the\n``.yaml`` input. For information about fine-tuning, please refer to [this tutorial on\nfine-tuning](../../getting-started/finetuning-example.rst).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
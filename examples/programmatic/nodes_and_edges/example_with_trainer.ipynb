{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import metatensor.torch as mts\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from metatrain.experimental.nanopet_basis import NanoPETBasis\n",
    "from metatrain.experimental.nanopet_basis import elearn\n",
    "from metatrain.utils.architectures import get_default_hypers\n",
    "from metatrain.utils.data import read_systems\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===== Generate target data - in this an EquivariantPowerSpectrum ===== ###\n",
    "\n",
    "# SphEx hypers\n",
    "sphex_hypers = {\n",
    "    \"cutoff\": {\"radius\": 3.5, \"smoothing\": {\"type\": \"ShiftedCosine\", \"width\": 0.1}},\n",
    "    \"density\": {\"type\": \"Gaussian\", \"width\": 0.3},\n",
    "    \"basis\": {\n",
    "        \"type\": \"TensorProduct\",\n",
    "        \"max_angular\": 5,\n",
    "        \"radial\": {\"type\": \"Gto\", \"max_radial\": 2},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Load systems\n",
    "n_frames = 10\n",
    "systems = read_systems(\"qm7x_reduced_100.xyz\")\n",
    "systems = [s.to(torch.float64) for s in systems][:n_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculators\n",
    "# eps_calc_1c = EquivariantPowerSpectrum(SphericalExpansion(**sphex_hypers))\n",
    "# eps_calc_2c = EquivariantPowerSpectrumByPair(\n",
    "#     SphericalExpansion(**sphex_hypers), SphericalExpansionByPair(**sphex_hypers)\n",
    "# )\n",
    "\n",
    "# # Compute EqPowSpecs\n",
    "# Lmax = 3\n",
    "# selected_keys = mts.Labels(\n",
    "#     [\"o3_lambda\"],\n",
    "#     torch.tensor([[o3_lambda] for o3_lambda in range(Lmax + 1)]).reshape(-1, 1),\n",
    "# )\n",
    "# target_node = eps_calc_1c.compute(\n",
    "#     systems,\n",
    "#     selected_keys=selected_keys,\n",
    "#     neighbors_to_properties=False,\n",
    "# )\n",
    "# target_edge = eps_calc_2c.compute(\n",
    "#     systems,\n",
    "#     selected_keys=selected_keys,\n",
    "#     neighbors_to_properties=False,\n",
    "# )\n",
    "\n",
    "# # Sum over neighbor types\n",
    "# target_node = target_node.keys_to_samples([\"neighbor_1_type\", \"neighbor_2_type\"])\n",
    "# target_node = mts.sum_over_samples(target_node, [\"neighbor_1_type\", \"neighbor_2_type\"])\n",
    "# # target_edge = target_edge.keys_to_properties([\"neighbor_1_type\", \"neighbor_2_type\"])\n",
    "\n",
    "# # Sum over neighbor types, permute dimensions, split nodes and edges\n",
    "# target_edge = target_edge.keys_to_samples([\"neighbor_1_type\"])\n",
    "# target_edge = elearn.drop_empty_blocks(target_edge)\n",
    "# target_edge = mts.sum_over_samples(target_edge, [\"neighbor_1_type\"])\n",
    "# target_edge = mts.permute_dimensions(target_edge, \"properties\", [2, 0, 3, 1])\n",
    "# target_edge = elearn.get_edges(target_edge)\n",
    "\n",
    "# # Save\n",
    "# mts.save(\"target_node.npz\", target_node)\n",
    "# mts.save(\"target_edge.npz\", target_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the total data\n",
    "target_node = mts.load(\"target_node.npz\")\n",
    "target_edge = mts.load(\"target_edge.npz\")\n",
    "\n",
    "# Select a subset of the target blocks - node\n",
    "# key_selection_node = target_node.keys.select(\n",
    "#     mts.Labels(\n",
    "#         [\"o3_lambda\", \"o3_sigma\", \"center_type\"],\n",
    "#         torch.tensor(\n",
    "#             [\n",
    "#                 # [0, 1, 6],\n",
    "#                 [1, 1, 6],\n",
    "#                 # [2, 1, 6],\n",
    "#                 # [0, 1, 1],\n",
    "#                 # [1, 1, 1],\n",
    "#                 # [2, 1, 1],\n",
    "#             ]\n",
    "#         ),\n",
    "#     )\n",
    "# )\n",
    "# selected_keys_node = mts.Labels(\n",
    "#     target_node.keys.names,\n",
    "#     target_node.keys.values[key_selection_node],\n",
    "# )\n",
    "# target_node = mts.TensorMap(\n",
    "#     selected_keys_node,\n",
    "#     [target_node[key] for key in selected_keys_node],\n",
    "# )\n",
    "\n",
    "# Triangularize in center type\n",
    "key_selection_edge = target_edge.keys.select(\n",
    "    mts.Labels(\n",
    "        [\"o3_lambda\", \"o3_sigma\", \"first_atom_type\", \"second_atom_type\"],\n",
    "        torch.tensor(\n",
    "            target_edge.keys.values[\n",
    "                target_edge.keys.values[:, 2] <= target_edge.keys.values[:, 3]\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "# Select a subset of the target blocks - edge\n",
    "# key_selection_edge = target_edge.keys.select(\n",
    "#     mts.Labels(\n",
    "#         [\"o3_lambda\", \"o3_sigma\", \"first_atom_type\", \"second_atom_type\"],\n",
    "#         torch.tensor(\n",
    "#             [\n",
    "#                 # [0, 1, 6, 6],\n",
    "#                 # [1, 1, 6, 6],\n",
    "#                 # [2, 1, 6, 6],\n",
    "#                 # [0, 1, 1, 6],\n",
    "#                 [1, 1, 1, 6],\n",
    "#                 # [2, 1, 1, 6],\n",
    "#                 # [8, 1, 1, 6],\n",
    "#             ]\n",
    "#         ),\n",
    "#     )\n",
    "# )\n",
    "selected_keys_edge = mts.Labels(\n",
    "    target_edge.keys.names,\n",
    "    target_edge.keys.values[key_selection_edge],\n",
    ")\n",
    "target_edge = mts.TensorMap(\n",
    "    selected_keys_edge,\n",
    "    [target_edge[key] for key in selected_keys_edge],\n",
    ")\n",
    "\n",
    "# Get the metadata for initializing PET\n",
    "metadata = {\n",
    "    \"in_keys_node\": target_node.keys,\n",
    "    \"in_keys_edge\": target_edge.keys,\n",
    "    \"out_properties_node\": [block.properties for block in target_node],\n",
    "    \"out_properties_edge\": [block.properties for block in target_edge],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train val split\n",
    "n_train, n_val = 15, 5\n",
    "all_id = np.arange(n_frames)\n",
    "rng = np.random.default_rng()\n",
    "rng.shuffle(all_id)\n",
    "train_id = all_id[:n_train].tolist()\n",
    "val_id = all_id[n_train : n_train + n_val].tolist()\n",
    "\n",
    "# Datasets\n",
    "train_dataset = elearn.get_dataset(systems, train_id, target_node, target_edge)\n",
    "val_dataset = elearn.get_dataset(systems, val_id, target_node, target_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target standardizers\n",
    "all_target_node = mts.join(\n",
    "    [train_dataset[i].node for i in range(len(train_id))],\n",
    "    \"samples\",\n",
    "    remove_tensor_name=True,\n",
    "    different_keys=\"union\",\n",
    ")\n",
    "all_target_edge = mts.join(\n",
    "    [train_dataset[i].edge for i in range(len(train_id))],\n",
    "    \"samples\",\n",
    "    remove_tensor_name=True,\n",
    "    different_keys=\"union\",\n",
    ")\n",
    "standardizers = {\n",
    "    \"node_mean\": elearn.get_tensor_invariant_mean(all_target_node),\n",
    "    \"node_std\": elearn.get_tensor_std(all_target_node),\n",
    "    \"edge_mean\": elearn.get_tensor_invariant_mean(all_target_edge),\n",
    "    \"edge_std\": elearn.get_tensor_std(all_target_edge),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from metatrain.utils.data.dataset import DatasetInfo\n",
    "from metatrain.utils.data.target_info import TargetInfo\n",
    "\n",
    "from metatrain.experimental.nanopet_basis import Trainer\n",
    "from metatrain.experimental.nanopet_basis.loss import L2Loss\n",
    "from metatrain.experimental.nanopet_basis.utils import get_node_layout, get_edge_layout\n",
    "\n",
    "\n",
    "# Modify model hypers\n",
    "hypers = get_default_hypers(\"experimental.nanopet_basis\")\n",
    "hypers[\"model\"][\"nanopet\"][\"cutoff\"] = sphex_hypers[\"cutoff\"][\"radius\"]\n",
    "hypers[\"model\"][\"nanopet\"][\"cutoff_width\"] = 0.1\n",
    "hypers[\"model\"][\"nanopet\"][\"d_pet\"] = 64\n",
    "hypers[\"model\"][\"basis\"][\"head_hidden_layer_widths\"] = [64, 64, 64]\n",
    "\n",
    "# Modify training hypers\n",
    "hypers[\"training\"][\"batch_size\"] = 1\n",
    "hypers[\"training\"][\"num_epochs\"] = 3\n",
    "hypers[\"training\"][\"log_interval\"] = 1\n",
    "\n",
    "pprint.pprint(hypers)\n",
    "\n",
    "# Define dataset info\n",
    "dataset_info = DatasetInfo(\n",
    "    length_unit=\"angstrom\",\n",
    "    atomic_types=[1, 6, 7, 16],\n",
    "    targets={\n",
    "        \"node\": TargetInfo(\n",
    "            quantity=\"node\",\n",
    "            layout=get_node_layout(metadata[\"in_keys_node\"], metadata[\"out_properties_node\"]),\n",
    "        ),\n",
    "        \"edge\": TargetInfo(\n",
    "            quantity=\"edge\",\n",
    "            layout=get_edge_layout(metadata[\"in_keys_edge\"], metadata[\"out_properties_edge\"]),\n",
    "        ),\n",
    "    }, \n",
    ")\n",
    "model = NanoPETBasis(\n",
    "    model_hypers=hypers[\"model\"],\n",
    "    dataset_info=dataset_info,\n",
    "    standardizers=standardizers,\n",
    ")\n",
    "trainer = Trainer(hypers[\"training\"])\n",
    "loss_fn = L2Loss()\n",
    "Path(\"ckpt\").mkdir(exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s : %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    model,\n",
    "    dtype=torch.float64,\n",
    "    devices=[torch.device(\"cpu\")],   \n",
    "    train_datasets=[train_dataset],\n",
    "    val_datasets=[val_dataset],\n",
    "    checkpoint_dir=\"ckpt\",\n",
    "    loss_fn=loss_fn,\n",
    ")\n",
    "# model = model.export()\n",
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart training\n",
    "restart_epoch = 4\n",
    "model = NanoPETBasis.load_checkpoint(f\"ckpt/model_{restart_epoch}.ckpt\")\n",
    "model = model.restart(dataset_info)\n",
    "\n",
    "trainer = Trainer.load_checkpoint(\n",
    "    f\"ckpt/model_{restart_epoch}.ckpt\", hypers[\"train\"]\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    model,\n",
    "    dtype=torch.float64,\n",
    "    devices=[torch.device(\"cpu\")],   \n",
    "    train_datasets=[train_dataset],\n",
    "    val_datasets=[val_dataset],\n",
    "    checkpoint_dir=\"ckpt\",\n",
    "    loss_fn=loss_fn,\n",
    ")\n",
    "\n",
    "# Export model\n",
    "# model = model.export()\n",
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training curve\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(np.arange(len(train_losses)), train_losses, label=\"train\", c=\"blue\")\n",
    "# ax.plot(np.arange(len(val_losses)), val_losses, label=\"val\", c=\"orange\")\n",
    "# ax.plot(np.arange(len(val_losses_rot)), val_losses_rot, label=\"val_rot\", c=\"red\")\n",
    "# ax.set_xscale(\"log\")\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_losses, \"experiments/0_epsbp/no_aug/train_losses.pt\")\n",
    "# torch.save(val_losses, \"experiments/0_epsbp/no_aug/val_losses.pt\")\n",
    "# torch.save(val_losses_rot, \"experiments/0_epsbp/no_aug/val_losses_rot.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make evaluation predictions\n",
    "# model.eval()\n",
    "\n",
    "# # Get systems\n",
    "# systems_train = [train_dataset.get_sample(A).systems for A in train_id]\n",
    "# systems_val = [val_dataset.get_sample(A).systems for A in val_id]\n",
    "\n",
    "# # Define a random transformation for each training system\n",
    "# rotations, inversions = elearn.get_system_transformations(systems_val)\n",
    "\n",
    "# # Get node targets\n",
    "# if model.in_keys_node is not None:\n",
    "#     train_targets_node = mts.join(\n",
    "#         [train_dataset.get_sample(A).targets_node for A in train_id],\n",
    "#         \"samples\",\n",
    "#         remove_tensor_name=True,\n",
    "#         different_keys=\"union\",\n",
    "#     )\n",
    "#     val_targets_node = mts.join(\n",
    "#         [val_dataset.get_sample(A).targets_node for A in val_id],\n",
    "#         \"samples\",\n",
    "#         remove_tensor_name=True,\n",
    "#         different_keys=\"union\",\n",
    "#     )\n",
    "#     systems_val_rot, val_targets_node_rot = rotational_augmenter.apply_augmentations(\n",
    "#         systems_val,\n",
    "#         {\"mtt::target_node\": val_targets_node},\n",
    "#         rotations,\n",
    "#         inversions,\n",
    "#     )\n",
    "#     val_targets_node_rot = val_targets_node_rot[\"mtt::target_node\"]\n",
    "\n",
    "# # Get edge targets\n",
    "# if model.in_keys_edge is not None:\n",
    "#     train_targets_edge = mts.join(\n",
    "#         [train_dataset.get_sample(A).targets_edge for A in train_id],\n",
    "#         \"samples\",\n",
    "#         remove_tensor_name=True,\n",
    "#         different_keys=\"union\",\n",
    "#     )\n",
    "#     val_targets_edge = mts.join(\n",
    "#         [val_dataset.get_sample(A).targets_edge for A in val_id],\n",
    "#         \"samples\",\n",
    "#         remove_tensor_name=True,\n",
    "#         different_keys=\"union\",\n",
    "#     )\n",
    "#     systems_val_rot, val_targets_edge_rot = rotational_augmenter.apply_augmentations(\n",
    "#         systems_val_rot,\n",
    "#         {\"mtt::target_edge\": val_targets_edge},\n",
    "#         rotations,\n",
    "#         inversions,\n",
    "#     )\n",
    "#     val_targets_edge_rot = val_targets_edge_rot[\"mtt::target_edge\"]\n",
    "#     train_targets_edge = mts.sort(train_targets_edge, \"samples\")\n",
    "#     val_targets_edge = mts.sort(val_targets_edge, \"samples\")\n",
    "#     val_targets_edge_rot = mts.sort(val_targets_edge_rot, \"samples\")\n",
    "\n",
    "# # Get predictions\n",
    "# train_predictions_node, train_predictions_edge = model(\n",
    "#     systems_train,\n",
    "#     train_id,\n",
    "# )\n",
    "# val_predictions_node, val_predictions_edge = model(\n",
    "#     systems_val,\n",
    "#     val_id,\n",
    "# )\n",
    "# val_predictions_node_rot, val_predictions_edge_rot = model(\n",
    "#     systems_val_rot,\n",
    "#     val_id,\n",
    "# )\n",
    "# if model.in_keys_node is not None:\n",
    "#     mts.equal_metadata_raise(train_targets_node, train_predictions_node)\n",
    "#     mts.equal_metadata_raise(val_targets_node, val_predictions_node)\n",
    "#     mts.equal_metadata_raise(val_targets_node_rot, val_predictions_node_rot)\n",
    "# if model.in_keys_edge is not None:\n",
    "#     mts.equal_metadata_raise(train_targets_edge, train_predictions_edge)\n",
    "#     mts.equal_metadata_raise(val_targets_edge, val_predictions_edge)\n",
    "#     mts.equal_metadata_raise(val_targets_edge_rot, val_predictions_edge_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "row_idxs = {}\n",
    "flat_idx = 0\n",
    "for alpha in atomic_types:\n",
    "    row_idxs[alpha] = flat_idx\n",
    "    flat_idx += 1\n",
    "\n",
    "for alpha_1 in atomic_types:\n",
    "    for alpha_2 in atomic_types:\n",
    "        if alpha_1 <= alpha_2:\n",
    "            row_idxs[(alpha_1, alpha_2)] = flat_idx\n",
    "            flat_idx += 1\n",
    "\n",
    "fig_1, axes_1 = plt.subplots(1, 3, figsize=(10, 5))\n",
    "fig_2, axes_2 = plt.subplots(len(row_idxs), (Lmax + 1) * 2 - 1, figsize=(50, 50))\n",
    "\n",
    "# Plot nodes\n",
    "ax_1_lim = 0\n",
    "if model.in_keys_node is not None:\n",
    "    for key in train_targets_node.keys:\n",
    "        ax_2 = axes_2[\n",
    "            row_idxs[key[\"center_type\"]],\n",
    "            key[\"o3_lambda\"] + (0 if key[\"o3_sigma\"] == 1 else Lmax),\n",
    "        ]\n",
    "\n",
    "        train_x = train_targets_node[key].values.flatten().detach().numpy()\n",
    "        train_y = train_predictions_node[key].values.flatten().detach().numpy()\n",
    "        val_x = val_targets_node[key].values.flatten().detach().numpy()\n",
    "        val_y = val_predictions_node[key].values.flatten().detach().numpy()\n",
    "        val_rot_x = val_targets_node_rot[key].values.flatten().detach().numpy()\n",
    "        val_rot_y = val_predictions_node_rot[key].values.flatten().detach().numpy()\n",
    "        # Set axes limits\n",
    "        ax_lim = np.max(np.abs(np.concatenate([train_x, val_x, train_y, val_y]))) * 1.1\n",
    "        ax_1_lim = max(ax_1_lim, ax_lim)\n",
    "        ax_2.set_xlim([-ax_lim, ax_lim])\n",
    "        ax_2.set_ylim([-ax_lim, ax_lim])\n",
    "        # for ax in [ax_1, ax_2]:\n",
    "        axes_1[0].scatter(train_x, train_y, alpha=0.3, marker=\".\", c=\"blue\")\n",
    "        axes_1[1].scatter(val_x, val_y, alpha=0.3, marker=\".\", c=\"orange\")\n",
    "        axes_1[2].scatter(val_rot_x, val_rot_y, alpha=0.3, marker=\".\", c=\"red\")\n",
    "        [ax_1.axline([0, 0], slope=1, linestyle=\"--\", c=\"gray\") for ax_1 in axes_1]\n",
    "        # ax_2.scatter(train_x, train_y, alpha=0.3, marker=\".\", c=\"blue\")\n",
    "        # ax_2.scatter(val_x, val_y, alpha=0.3, marker=\".\", c=\"orange\")\n",
    "        ax_2.axline([0, 0], slope=1, linestyle=\"--\", c=\"gray\")\n",
    "\n",
    "# Plot edges\n",
    "if model.in_keys_edge is not None:\n",
    "    for key in train_targets_edge.keys:\n",
    "        ax_2 = axes_2[\n",
    "            row_idxs[(key[\"first_atom_type\"], key[\"second_atom_type\"])],\n",
    "            key[\"o3_lambda\"] + (0 if key[\"o3_sigma\"] == 1 else Lmax),\n",
    "        ]\n",
    "\n",
    "        train_x = train_targets_edge[key].values.flatten().detach().numpy()\n",
    "        train_y = train_predictions_edge[key].values.flatten().detach().numpy()\n",
    "        val_x = val_targets_edge[key].values.flatten().detach().numpy()\n",
    "        val_y = val_predictions_edge[key].values.flatten().detach().numpy()\n",
    "        ax_lim = np.max(np.abs(np.concatenate([train_x, val_x, train_y, val_y]))) * 1.1\n",
    "        ax_1_lim = max(ax_1_lim, ax_lim)\n",
    "        ax_2.set_xlim([-ax_lim, ax_lim])\n",
    "        ax_2.set_ylim([-ax_lim, ax_lim])\n",
    "        # for ax in [ax_1, ax_2]:\n",
    "        axes_1[0].scatter(train_x, train_y, alpha=0.3, marker=\".\", c=\"blue\")\n",
    "        axes_1[1].scatter(val_x, val_y, alpha=0.3, marker=\".\", c=\"orange\")\n",
    "        axes_1[2].scatter(val_rot_x, val_rot_y, alpha=0.3, marker=\".\", c=\"red\")\n",
    "        [ax_1.axline([0, 0], slope=1, linestyle=\"--\", c=\"gray\") for ax_1 in axes_1]\n",
    "        # ax_2.scatter(train_x, train_y, alpha=0.3, marker=\".\", c=\"blue\")\n",
    "        # ax_2.scatter(val_x, val_y, alpha=0.3, marker=\".\", c=\"orange\")\n",
    "        ax_2.axline([0, 0], slope=1, linestyle=\"--\", c=\"gray\")\n",
    "\n",
    "# Format ax_1\n",
    "[ax_1.set_xlim([-ax_1_lim, ax_1_lim]) for ax_1 in axes_1]\n",
    "[ax_1.set_ylim([-ax_1_lim, ax_1_lim]) for ax_1 in axes_1]\n",
    "axes_1[-1].scatter([None], [None], label=\"train\", c=\"blue\")\n",
    "axes_1[-1].scatter([None], [None], label=\"val\", c=\"orange\")\n",
    "axes_1[-1].scatter([None], [None], label=\"val_rot\", c=\"red\")\n",
    "axes_1[-1].legend()\n",
    "\n",
    "# Format axes_2\n",
    "axes_2[0, -1].scatter([None], [None], label=\"train\", c=\"blue\")\n",
    "axes_2[0, -1].scatter([None], [None], label=\"val\", c=\"orange\")\n",
    "axes_2[0, -1].legend()\n",
    "for key, idx in row_idxs.items():\n",
    "    axes_2[idx, 0].set_ylabel(f\"atom = {key}\")\n",
    "[\n",
    "    ax.set_title(f\"lam = {lam}, sig = {sig}\")\n",
    "    for lam, sig, ax in zip(\n",
    "        list(range(Lmax + 1)) + list(range(1, Lmax + 1)),\n",
    "        ([1] * (Lmax + 1)) + ([-1] * (Lmax + 1)),\n",
    "        axes_2[0, :],\n",
    "    )\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivariance evaluation\n",
    "model.eval()\n",
    "\n",
    "systems_train = [train_dataset.get_sample(A).systems for A in train_id]\n",
    "systems_val = [val_dataset.get_sample(A).systems for A in val_id]\n",
    "\n",
    "# Get targets\n",
    "if model.in_keys_node is not None:\n",
    "    train_targets_node = mts.join(\n",
    "        [train_dataset.get_sample(A).targets_node for A in train_id],\n",
    "        \"samples\",\n",
    "        remove_tensor_name=True,\n",
    "        different_keys=\"union\",\n",
    "    )\n",
    "    val_targets_node = mts.join(\n",
    "        [val_dataset.get_sample(A).targets_node for A in val_id],\n",
    "        \"samples\",\n",
    "        remove_tensor_name=True,\n",
    "        different_keys=\"union\",\n",
    "    )\n",
    "\n",
    "if model.in_keys_edge is not None:\n",
    "    train_targets_edge = mts.join(\n",
    "        [train_dataset.get_sample(A).targets_edge for A in train_id],\n",
    "        \"samples\",\n",
    "        remove_tensor_name=True,\n",
    "        different_keys=\"union\",\n",
    "    )\n",
    "    val_targets_edge = mts.join(\n",
    "        [val_dataset.get_sample(A).targets_edge for A in val_id],\n",
    "        \"samples\",\n",
    "        remove_tensor_name=True,\n",
    "        different_keys=\"union\",\n",
    "    )\n",
    "    train_targets_edge = mts.sort(train_targets_edge, \"samples\")\n",
    "    val_targets_edge = mts.sort(val_targets_edge, \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment systems and targets with rotations around Z\n",
    "losses = {\"train_node\": [], \"train_edge\": [], \"val_node\": [], \"val_edge\": []}\n",
    "Z = np.linspace(0, 2 * np.pi, 7)\n",
    "\n",
    "tmp_systems_train = []\n",
    "tmp_systems_val = []\n",
    "\n",
    "for z in Z:\n",
    "    # Define transformations\n",
    "    rotations = [Rotation.from_euler(\"ZYZ\", [0, 0, z]) for _ in systems_train]\n",
    "    inversions = [1 for _ in systems_train]\n",
    "\n",
    "    # TRAIN: Apply rotational augmentation - node\n",
    "    if model.in_keys_node is not None:\n",
    "        systems_train_, train_targets_node_ = rotational_augmenter.apply_augmentations(\n",
    "            systems_train,\n",
    "            {\"mtt::target_node\": train_targets_node},\n",
    "            rotations,\n",
    "            inversions,\n",
    "        )\n",
    "        train_targets_node_ = train_targets_node_[\"mtt::target_node\"]\n",
    "\n",
    "    # TRAIN: Apply rotational augmentation - edge\n",
    "    if model.in_keys_edge is not None:\n",
    "        systems_train_, train_targets_edge_ = rotational_augmenter.apply_augmentations(\n",
    "            systems_train,\n",
    "            {\"mtt::target_edge\": train_targets_edge},\n",
    "            rotations,\n",
    "            inversions,\n",
    "        )\n",
    "        train_targets_edge_ = train_targets_edge_[\"mtt::target_edge\"]\n",
    "\n",
    "    tmp_systems_train.append(systems_train_[0])\n",
    "\n",
    "    # TRAIN: Get predictions\n",
    "    train_predictions_node_, train_predictions_edge_ = model(\n",
    "        systems_train_,\n",
    "        train_id,\n",
    "    )\n",
    "    assert mts.equal_metadata(train_targets_node, train_targets_node_)\n",
    "    assert mts.equal_metadata(train_targets_edge, train_targets_edge_)\n",
    "    if z not in [0, 2 * np.pi]:  # TensorMaps should not be equal\n",
    "        assert not mts.allclose(train_targets_node, train_targets_node_)\n",
    "        assert not mts.allclose(train_targets_edge, train_targets_edge_)\n",
    "    assert mts.equal_metadata(train_predictions_node_, train_targets_node_)\n",
    "    assert mts.equal_metadata(train_predictions_edge_, train_targets_edge_)\n",
    "\n",
    "    # VAL: Apply rotational augmentation - node\n",
    "    if model.in_keys_node is not None:\n",
    "        systems_val_, val_targets_node_ = rotational_augmenter.apply_augmentations(\n",
    "            systems_val,\n",
    "            {\"mtt::target_node\": val_targets_node},\n",
    "            rotations,\n",
    "            inversions,\n",
    "        )\n",
    "        val_targets_node_ = val_targets_node_[\"mtt::target_node\"]\n",
    "\n",
    "    # VAL: Apply rotational augmentation - edge\n",
    "    if model.in_keys_edge is not None:\n",
    "        systems_val_, val_targets_edge_ = rotational_augmenter.apply_augmentations(\n",
    "            systems_val,\n",
    "            {\"mtt::target_edge\": val_targets_edge},\n",
    "            rotations,\n",
    "            inversions,\n",
    "        )\n",
    "        val_targets_edge_ = val_targets_edge_[\"mtt::target_edge\"]\n",
    "\n",
    "    tmp_systems_val.append(systems_val_[0])\n",
    "\n",
    "    # VAL: Get predictions\n",
    "    val_predictions_node_, val_predictions_edge_ = model(\n",
    "        systems_val_,\n",
    "        val_id,\n",
    "    )\n",
    "    assert mts.equal_metadata(val_targets_node, val_targets_node_)\n",
    "    assert mts.equal_metadata(val_targets_edge, val_targets_edge_)\n",
    "    if z not in [0, 2 * np.pi]:  # TensorMaps should not be equal\n",
    "        assert not mts.allclose(val_targets_node, val_targets_node_)\n",
    "        assert not mts.allclose(val_targets_edge, val_targets_edge_)\n",
    "    assert mts.equal_metadata(val_predictions_node_, val_targets_node_)\n",
    "    assert mts.equal_metadata(val_predictions_edge_, val_targets_edge_)\n",
    "\n",
    "    # Compute losses and store\n",
    "    losses[\"train_node\"].append(\n",
    "        elearn.l2loss(train_predictions_node_, train_targets_node_)\n",
    "        / len(systems_train_)\n",
    "    )\n",
    "    losses[\"train_edge\"].append(\n",
    "        elearn.l2loss(train_predictions_edge_, train_targets_edge_)\n",
    "        / len(systems_train_)\n",
    "    )\n",
    "    losses[\"val_node\"].append(\n",
    "        elearn.l2loss(val_predictions_node_, val_targets_node_) / len(systems_val_)\n",
    "    )\n",
    "    losses[\"val_edge\"].append(\n",
    "        elearn.l2loss(val_predictions_edge_, val_targets_edge_) / len(systems_val_)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss as a function of Z rotation\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10, 5))\n",
    "axes[0].plot(\n",
    "    Z, [i.item() for i in losses[\"train_node\"]], label=\"train_node\", marker=\".\"\n",
    ")\n",
    "axes[0].plot(Z, [i.item() for i in losses[\"val_node\"]], label=\"val_node\", marker=\".\")\n",
    "axes[1].plot(\n",
    "    Z, [i.item() for i in losses[\"train_edge\"]], label=\"train_edge\", marker=\".\"\n",
    ")\n",
    "axes[1].plot(Z, [i.item() for i in losses[\"val_edge\"]], label=\"val_edge\", marker=\".\")\n",
    "[ax.legend() for ax in axes]\n",
    "[ax.set_xlabel(\"Rotation around Z / rad\") for ax in axes]\n",
    "[ax.set_ylabel(\"Loss per structure\") for ax in axes];\n",
    "# [ax.set_yscale(\"log\") for ax in axes];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase\n",
    "import chemiscope\n",
    "\n",
    "\n",
    "tmp_frames = [\n",
    "    ase.Atoms(\n",
    "        system.types,\n",
    "        system.positions,\n",
    "    )\n",
    "    for system in tmp_systems_train\n",
    "]\n",
    "chemiscope.show(tmp_frames, mode=\"structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pet_basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

architecture:
  name: pet  # the architecture name for PET, or the name for other architectures, if you want to train other models
  model:
    cutoff: 4.5  # the cutoff
  training:
    num_epochs: 10  # this is for a reasonable time of a tutorial, for a good model, consider increasing the number
    batch_size: 10  # the size of the training data feed to the model per batch, determining the GPU memory usage during the training
    log_interval: 1
    checkpoint_interval: 10  # it saves checkpoints of the model every 10 epochs

# this needs specifying based on the specific dataset
training_set:
  systems:
    read_from: ../../../../examples/ase/ethanol_reduced_100.xyz
    length_unit: Angstrom
  targets:
    energy:
      key: energy  # name of the target value
      unit: eV  # unit of the target value

test_set: 0.1  # 10 % of the training_set are randomly split and taken for test set
validation_set: 0.1  # 10 % of the training_set are randomly split and for validation set
